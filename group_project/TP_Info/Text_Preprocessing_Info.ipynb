{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9799167-17bc-4304-a440-ccbc257f88a3",
   "metadata": {},
   "source": [
    "### 1. 环境准备\n",
    "Purpose: Load the core language resources required for preprocessing.\n",
    "Key point: Ensure the local availability of basic dictionaries and machine learning models.\n",
    "\n",
    "``` Python\n",
    "nltk.download('punkt')   # 分词模型\n",
    "nltk.download('stopwords')  # 停用词库\n",
    "nltk.download('wordnet')    # 词形还原词典\n",
    "nltk.download('averaged_perceptron_tagger')  # 词性标注模型\n",
    "```\n",
    "\n",
    "### 2. 流程框架\n",
    "Modular Design: Separate text processing from file operation logic.\n",
    "Scalability: Allow independent optimization of text processing or I/O workflows.\n",
    "\n",
    "``` Python\n",
    "def preprocess_text(text):\n",
    "    # 8 core processing steps...\n",
    "def process_articles(input_dir, output_dir):\n",
    "    # File traversal and data integration...\n",
    "```\n",
    "\n",
    "### 3. 文本清洗核心步骤\n",
    "\n",
    "#### 3.1 原始文本净化\n",
    "``` Python\n",
    "text = re.sub(r'<.*?>', '', text)  # 去除HTML标签\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text)  # 移除非字母字符\n",
    "```\n",
    "Example: \"Biden's 2023 plan <br>...\" → \"Bidens plan\"\n",
    "\n",
    "#### 3.2 智能分词\n",
    "``` Python\n",
    "tokenizer = RegexpTokenizer(r'\\b[a-zA-Z-]{2,}\\b')  # 保留≥2字母的单词 \n",
    "tokens = tokenizer.tokenize(text.lower())\n",
    "```\n",
    "但是这里第一条不能删除单个字母，在后续会再次处理\n",
    "\n",
    "#### 3.3 上下文感知停用词过滤\n",
    "Innovation: Customize the filter word library based on domain knowledge (international political news)\n",
    "Linguistic considerations: Remove meaningless prepositional structures\n",
    "\n",
    "``` Python  \n",
    "stop_words = set(stopwords.words('english'))     # 英语（english）的停用词\n",
    "tokens = [word for word in tokens if word not in stop_words]     #总共198个，package所包含的\n",
    "``` \n",
    "根据向量情况反馈，清洗不够干净，有所遗留\n",
    "\n",
    "因此`custom`了两次：\n",
    "``` Python\n",
    "custom_prepositions = {'above', 'across',...\n",
    "                       }\n",
    "custom_stop_words = {\n",
    "    'china', 'usa', 'jan', 'feb', ...  # 自定义政治/时间相关停用词\n",
    "}\n",
    "stop_words.update(custom_prepositions)\n",
    "stop_words.update(custom_prepositions)  # 添加50+介词\n",
    "```\n",
    "\n",
    "#### 3.4 词形还原优化\n",
    "``` Python \n",
    "pos_tags = pos_tag(tokens)  # 词性标注\n",
    "wn_tag = get_wordnet_pos(tag)  # 转换标注体系\n",
    "lemmatizer.lemmatize(word, wn_tag)  # 基于词性的还原\n",
    "```\n",
    "\n",
    "#### 3.5 特殊处理，remove specific words\n",
    "1. Remove words longer than 10 letters：\n",
    "   经过最初的几次试验，发现‘body’部分每篇处理完的文章都有一个很长的单词，推测应该属于与文章不相关的其他部分，选择删除\n",
    "``` Python \n",
    "tokens = [word for word in tokens if len(word) <= 10]\n",
    "```\n",
    "2. Remove specific words (performed after lemmatization):\n",
    "   同上，发现每篇文章都有相同的word，属于干扰因素，直接选择删去(V1)；\n",
    "   但是，在试过一版的时候，发现有遗留`{'principle', 'open', 'new'}`，更改代码为after lemmatization之后再处理(V2);\n",
    "   最后，根据向量反馈，删除数字相关。\n",
    "``` Python \n",
    "words_to_remove = {'licensing', 'right',\n",
    "                  }\n",
    " words_to_remove.update(['one', \n",
    "                         )\n",
    "```\n",
    "3. Remove single-letter words：\n",
    "   3.2已经提到过使用去除单个字母，但是在body部分依旧残留很多`‘u’`\n",
    "``` Python \n",
    "lemmatized_tokens = [word for word in lemmatized_tokens if len(word) >= 2]\n",
    "```\n",
    "关键修改说明：\n",
    "在预处理函数的最后（返回`lemmatized_tokens`之前），添加了一个过滤步骤，确保所有保留的单词长度至少为2。这样可以有效移除在词形还原等步骤中可能生成的单个字母（如`'u'`），确保输出结果中不再包含这类单词。\n",
    "\n",
    "### 4. 数据流水线设计\n",
    "process_articles() 函数工作流程：\n",
    "1. 递归遍历目录树\n",
    "2. JSON文件解析与语言检测（隐式通过内容存在性检查）\n",
    "3. 标题/正文独立处理\n",
    "4. 基于标题的重复检测\n",
    "5. 结构化存储为CSV\n",
    "\n",
    "输出格式优化：\n",
    "``` csv\n",
    "\"title\", \"date\", \"body\"\n",
    "\"us, china, trade...\", \"2023-01-01\", \"negotiation, tariff...\"\n",
    "```\n",
    "\n",
    "### 5. 创新性设计亮点\n",
    "混合停用词策略：\n",
    "基础停用词(179) + 自定义停用词(120+) + 介词库(50+)\n",
    "\n",
    "领域定制过滤：\n",
    "政治实体词（china/usa/russia）\n",
    "时间相关词（jan/mon/week）\n",
    "\n",
    "预处理后验证：\n",
    "空内容检测、标题重复检测\n",
    "\n",
    "#### 6. P.S. The directory must contain a folder named `'articles'` that includes the JSON files, where the collected text is included. After running the code, a folder named `'processed_articles'` will be automatically created, and the output CSV file will be saved in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b422a814-07c2-4761-87bc-6f9825cdd71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
