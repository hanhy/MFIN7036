{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dff8baf-7575-490a-8024-c7fff0856cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to CSV file: processed_articles/processed_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv \n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map Treebank POS tags to WordNet POS tags for accurate lemmatization.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def preprocess_text(text):\n",
    "    # 1. Clean the text: remove HTML tags, special characters, and numbers\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 2. Tokenization\n",
    "    tokenizer = RegexpTokenizer(r'\\b[a-zA-Z-]{2,}\\b')  # Only keep words with â‰¥ 2 letters\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    # 3. Remove 'reuters' tokens (case-insensitive)\n",
    "    tokens = [word for word in tokens if word != 'reuters']\n",
    "\n",
    "    # 4. Remove custom stop words (prepositions) and standard stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_prepositions = {'above', 'across', 'after', 'against', 'along', 'among', \n",
    "                           'around', 'at', 'before', 'behind', 'below', 'beneath', \n",
    "                           'beside', 'between', 'beyond', 'by', 'despite', 'down', \n",
    "                           'except', 'for', 'from', 'in', 'inside', 'into', 'near', \n",
    "                           'of', 'off', 'on', 'onto', 'out', 'over', 'past', \n",
    "                           'regarding', 'round', 'since', 'through', 'throughout', \n",
    "                           'till', 'to', 'toward', 'under', 'underneath', 'until', \n",
    "                           'unto', 'up', 'upon', 'with', 'within', 'without'}\n",
    "\n",
    "    custom_stop_words = {  \n",
    "        'china', 'usa', 'us', 'america', 'american', 'americans', 'chinese', 'russia', \n",
    "        'russian', 'putin', 'vladimir', 'trump', 'donald', 'biden', 'joe', 'ukraine', \n",
    "        'ukrainian', 'ukrainians', 'ukraines', 'say', 'jan', 'feb', 'mar', 'apr', 'may', \n",
    "        'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'mon', 'tue', 'wed', 'thu', 'fri', \n",
    "        'sat', 'sun', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', \n",
    "        'sunday', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', \n",
    "        'september', 'october', 'november', 'december', 'today', 'yesterday', 'tomorrow', \n",
    "        'week', 'month', 'year', 'time', 'day', 'weekend', 'morning', 'afternoon', \n",
    "        'evening', 'night', 'news', 'new'\n",
    "    }\n",
    "\n",
    "    stop_words.update(custom_prepositions)\n",
    "    stop_words.update(custom_stop_words)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 5. Remove words longer than 10 letters\n",
    "    tokens = [word for word in tokens if len(word) <= 10]\n",
    "\n",
    "    # 6. POS tagging and lemmatization for accurate tense restoration\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag:\n",
    "            lemmatized = lemmatizer.lemmatize(word, wn_tag)\n",
    "        else:\n",
    "            lemmatized = lemmatizer.lemmatize(word)\n",
    "        lemmatized_tokens.append(lemmatized)\n",
    "\n",
    "    # 7. Remove specific words (performed after lemmatization)\n",
    "    words_to_remove = {'licensing', 'right', 'thomson', 'trust', 'tabsuggested', \n",
    "                   'principle', 'open', 'new', 'standard', 'say', 'co', 'ltd'}\n",
    "    words_to_remove.update(['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', \n",
    "                        'nine', 'ten', 'hundred', 'thousand', 'million', 'billion', 'trillion'])\n",
    "\n",
    "    lemmatized_tokens = [word for word in lemmatized_tokens if word not in words_to_remove]\n",
    "\n",
    "\n",
    "\n",
    "    #8. Remove single-letter words\n",
    "    lemmatized_tokens = [word for word in lemmatized_tokens if len(word) >= 2]\n",
    "\n",
    "    return lemmatized_tokens\n",
    "    \n",
    "def process_articles(input_directory, output_directory):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    articles = []  # List to store processed articles\n",
    "\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    article = json.load(f)\n",
    "\n",
    "                # Check if the article is in English\n",
    "                if not article.get('title') or not article.get('body'):\n",
    "                    continue\n",
    "\n",
    "                # Preprocess the title and body\n",
    "                article['title'] = preprocess_text(article['title'])\n",
    "                article['body'] = preprocess_text(article['body'])\n",
    "\n",
    "                # Check if the article is empty\n",
    "                if not article['title'] or not article['body']:\n",
    "                    continue\n",
    "\n",
    "                # Check for duplicates\n",
    "                if article['title'] in [a['title'] for a in articles]:\n",
    "                    continue\n",
    "\n",
    "                # Add the date field\n",
    "                article['date'] = article.get('date', 'Unknown')  # Use 'Unknown' if date is missing\n",
    "\n",
    "                articles.append(article)  # Add the processed article to the list\n",
    "\n",
    "    # Convert the list of articles to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # New step: Convert the body field's token list into a space-delimited string\n",
    "    df['body'] = df['body'].apply(lambda x: ', '.join(x))  # Key modification: space -> comma\n",
    "    df['title'] = df['title'].apply(lambda x: ', '.join(x))  # The title needs to be processed as well\n",
    "\n",
    "    # Keep only the title, date, and body columns\n",
    "    df = df[['title', 'date', 'body']]\n",
    "\n",
    "    # Build the output file path\n",
    "    output_path = os.path.join(output_directory, \"processed_articles.csv\")\n",
    "\n",
    "    # Save the processed data to a CSV file\n",
    "    df.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)  \n",
    "\n",
    "    print(f\"Processed data saved to CSV file: {output_path}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"articles\"  # Path to the directory containing the JSON files\n",
    "    output_directory = \"processed_articles\"  # Path to the output directory\n",
    "    process_articles(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50571076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_prepositions = {'above', 'across', 'after', 'against', 'along', 'among', \n",
    "                           'around', 'at', 'before', 'behind', 'below', 'beneath', \n",
    "                           'beside', 'between', 'beyond', 'by', 'despite', 'down', \n",
    "                           'except', 'for', 'from', 'in', 'inside', 'into', 'near', \n",
    "                           'of', 'off', 'on', 'onto', 'out', 'over', 'past', \n",
    "                           'regarding', 'round', 'since', 'through', 'throughout', \n",
    "                           'till', 'to', 'toward', 'under', 'underneath', 'until', \n",
    "                           'unto', 'up', 'upon', 'with', 'within', 'without'}\n",
    "\n",
    "custom_stop_words = {  \n",
    "        'china', 'usa', 'us', 'america', 'american', 'americans', 'chinese', 'russia', \n",
    "        'russian', 'putin', 'vladimir', 'trump', 'donald', 'biden', 'joe', 'ukraine', \n",
    "        'ukrainian', 'ukrainians', 'ukraines', 'say', 'jan', 'feb', 'mar', 'apr', 'may', \n",
    "        'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'mon', 'tue', 'wed', 'thu', 'fri', \n",
    "        'sat', 'sun', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', \n",
    "        'sunday', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', \n",
    "        'september', 'october', 'november', 'december', 'today', 'yesterday', 'tomorrow', \n",
    "        'week', 'month', 'year', 'time', 'day', 'weekend', 'morning', 'afternoon', \n",
    "        'evening', 'night', 'news', 'new'\n",
    "    }\n",
    "\n",
    "\n",
    "stop_words_list = [ 'shanghai','nichola','roger','msci','iran','states','italy','united','china','usa','us','america','american','americans','chinese','china','russia','russian','putin','vladimir','trump','donald','biden','joe','ukraine','ukrainian','ukrainians','ukraines','ukraine','say','jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec','mon','tue','wed','thu','fri','sat','sun','monday','tuesday','wednesday','thursday','friday','saturday','sunday','january','february','march','april','may','june','july','august','september','october','november','december','today','yesterday','tomorrow','week','month','year','time','day','weekend','morning','afternoon','evening','night','news','new','news']\n",
    "list_numbers = ['eoi','name','houthi','uae','euro','yen','instead','liga','len','nhl','one','two','three','four','five','six','seven','eight','nine','ten','hundred','thousand','million','billion','trillion','first','second','third','eighted','series','hong','kong','new','york','los','angeles','san','francisco','las','vegas','san','diego','san','jose']\n",
    "country_list = ['myanmar','robert','lebanon','iivi','william','zalando','olympic','country','world','africa','China', 'United States of America', 'United Kingdom', 'France', 'Germany', 'Japan', 'Russia', 'Australia', 'Canada', 'India', 'Brazil', 'Italy', 'Spain', 'South Korea', 'Mexico', 'Netherlands', 'Switzerland', 'Sweden', 'Norway', 'Denmark', 'Finland', 'Greece', 'Ireland', 'Portugal', 'Poland', 'Ukraine', 'Romania', 'Belgium', 'Austria', 'Turkey', 'Saudi Arabia', 'United Arab Emirates', 'Iran', 'Iraq', 'Israel', 'Egypt', 'South Africa', 'Argentina', 'Venezuela', 'Thailand', 'Malaysia', 'Singapore', 'Indonesia', 'Philippines', 'Pakistan', 'Bangladesh', 'Nigeria', 'Kenya', 'Tanzania', 'Uganda']\n",
    "list_append = [\"whose\", \"german\", \"saidsign\", \"ceo\",\"exar\",\"chos\",\"sme\", \"vietnam\", \"gsk\", \"mori\", \"queen\", \"threeyear\",\"would\", \"come\", \"also\", \"could\", \"edit\", \"include\",\"pitch\", \"Britain\", \"Indian\", \"collin\", \"koo\", \"skorea\", \"men\", \"koo\", \"hub\",\"bbva\", \"korea\", \"inc\", \"btp\", \"ntpcs\", \"telecom\", \"omi\",\"jen\",\"andre\", \"spac\", \"sabadell\",\"faa\", \"unicredit\", \"city\", \"georgia\", \"puma\", \"philip\", \"england\", \"tokyo\", \"announce\", \"safrica\", \"andrea\"]\n",
    "stop_words.update(stop_words_list)\n",
    "stop_words.update(list_numbers)\n",
    "stop_words.update(country_list)\n",
    "stop_words.update(list_append)\n",
    "stop_words.update(custom_prepositions)\n",
    "stop_words.update(custom_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d59409-7c35-441b-85ef-8cc2f48e4c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = len(stop_words)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51c264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
