{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dff8baf-7575-490a-8024-c7fff0856cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to CSV file: processed_articles/processed_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv \n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map Treebank POS tags to WordNet POS tags for accurate lemmatization.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def preprocess_text(text):\n",
    "    # 1. Clean the text: remove HTML tags, special characters, and numbers\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 2. Tokenization\n",
    "    tokenizer = RegexpTokenizer(r'\\b[a-zA-Z-]{2,}\\b')  # Only keep words with â‰¥ 2 letters\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    # 3. Remove 'reuters' tokens (case-insensitive)\n",
    "    tokens = [word for word in tokens if word != 'reuters']\n",
    "\n",
    "    # 4. Remove custom stop words (prepositions) and standard stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_prepositions = {'above', 'across', 'after', 'against', 'along', 'among', \n",
    "                           'around', 'at', 'before', 'behind', 'below', 'beneath', \n",
    "                           'beside', 'between', 'beyond', 'by', 'despite', 'down', \n",
    "                           'except', 'for', 'from', 'in', 'inside', 'into', 'near', \n",
    "                           'of', 'off', 'on', 'onto', 'out', 'over', 'past', \n",
    "                           'regarding', 'round', 'since', 'through', 'throughout', \n",
    "                           'till', 'to', 'toward', 'under', 'underneath', 'until', \n",
    "                           'unto', 'up', 'upon', 'with', 'within', 'without'}\n",
    "\n",
    "    custom_stop_words = {  \n",
    "        'china', 'usa', 'us', 'america', 'american', 'americans', 'chinese', 'russia', \n",
    "        'russian', 'putin', 'vladimir', 'trump', 'donald', 'biden', 'joe', 'ukraine', \n",
    "        'ukrainian', 'ukrainians', 'ukraines', 'say', 'jan', 'feb', 'mar', 'apr', 'may', \n",
    "        'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'mon', 'tue', 'wed', 'thu', 'fri', \n",
    "        'sat', 'sun', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', \n",
    "        'sunday', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', \n",
    "        'september', 'october', 'november', 'december', 'today', 'yesterday', 'tomorrow', \n",
    "        'week', 'month', 'year', 'time', 'day', 'weekend', 'morning', 'afternoon', \n",
    "        'evening', 'night', 'news', 'new'\n",
    "    }\n",
    "\n",
    "    stop_words.update(custom_prepositions)\n",
    "    stop_words.update(custom_stop_words)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 5. Remove words longer than 10 letters\n",
    "    tokens = [word for word in tokens if len(word) <= 10]\n",
    "\n",
    "    # 6. POS tagging and lemmatization for accurate tense restoration\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag:\n",
    "            lemmatized = lemmatizer.lemmatize(word, wn_tag)\n",
    "        else:\n",
    "            lemmatized = lemmatizer.lemmatize(word)\n",
    "        lemmatized_tokens.append(lemmatized)\n",
    "\n",
    "    # 7. Remove specific words (performed after lemmatization)\n",
    "    words_to_remove = {'licensing', 'right', 'thomson', 'trust', 'tabsuggested', \n",
    "                   'principle', 'open', 'new', 'standard', 'say', 'co', 'ltd'}\n",
    "    words_to_remove.update(['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', \n",
    "                        'nine', 'ten', 'hundred', 'thousand', 'million', 'billion', 'trillion'])\n",
    "\n",
    "    lemmatized_tokens = [word for word in lemmatized_tokens if word not in words_to_remove]\n",
    "\n",
    "\n",
    "\n",
    "    #8. Remove single-letter words\n",
    "    lemmatized_tokens = [word for word in lemmatized_tokens if len(word) >= 2]\n",
    "\n",
    "    return lemmatized_tokens\n",
    "    \n",
    "def process_articles(input_directory, output_directory):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    articles = []  # List to store processed articles\n",
    "\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    article = json.load(f)\n",
    "\n",
    "                # Check if the article is in English\n",
    "                if not article.get('title') or not article.get('body'):\n",
    "                    continue\n",
    "\n",
    "                # Preprocess the title and body\n",
    "                article['title'] = preprocess_text(article['title'])\n",
    "                article['body'] = preprocess_text(article['body'])\n",
    "\n",
    "                # Check if the article is empty\n",
    "                if not article['title'] or not article['body']:\n",
    "                    continue\n",
    "\n",
    "                # Check for duplicates\n",
    "                if article['title'] in [a['title'] for a in articles]:\n",
    "                    continue\n",
    "\n",
    "                # Add the date field\n",
    "                article['date'] = article.get('date', 'Unknown')  # Use 'Unknown' if date is missing\n",
    "\n",
    "                articles.append(article)  # Add the processed article to the list\n",
    "\n",
    "    # Convert the list of articles to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # New step: Convert the body field's token list into a space-delimited string\n",
    "    df['body'] = df['body'].apply(lambda x: ', '.join(x))  # Key modification: space -> comma\n",
    "    df['title'] = df['title'].apply(lambda x: ', '.join(x))  # The title needs to be processed as well\n",
    "\n",
    "    # Keep only the title, date, and body columns\n",
    "    df = df[['title', 'date', 'body']]\n",
    "\n",
    "    # Build the output file path\n",
    "    output_path = os.path.join(output_directory, \"processed_articles.csv\")\n",
    "\n",
    "    # Save the processed data to a CSV file\n",
    "    df.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)  \n",
    "\n",
    "    print(f\"Processed data saved to CSV file: {output_path}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"articles\"  # Path to the directory containing the JSON files\n",
    "    output_directory = \"processed_articles\"  # Path to the output directory\n",
    "    process_articles(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d59409-7c35-441b-85ef-8cc2f48e4c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
