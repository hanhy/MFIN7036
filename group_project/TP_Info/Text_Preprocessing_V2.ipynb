{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dff8baf-7575-490a-8024-c7fff0856cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\markham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\markham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\markham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\markham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\markham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 165\u001b[0m\n\u001b[0;32m    163\u001b[0m input_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2021\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Path to the directory containing the JSON files\u001b[39;00m\n\u001b[0;32m    164\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata2021\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Path to the output directory\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m process_articles(input_directory, output_directory)\n",
      "Cell \u001b[1;32mIn[3], line 120\u001b[0m, in \u001b[0;36mprocess_articles\u001b[1;34m(input_directory, output_directory)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    119\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    121\u001b[0m         article \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# Check if the article is in English\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\markham\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv \n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map Treebank POS tags to WordNet POS tags for accurate lemmatization.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def preprocess_text(text):\n",
    "    # 1. Clean the text: remove HTML tags, special characters, and numbers\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 2. Tokenization\n",
    "    tokenizer = RegexpTokenizer(r'\\b[a-zA-Z-]{2,}\\b')  # Only keep words with â‰¥ 2 letters\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    # 3. Remove 'reuters' tokens (case-insensitive)\n",
    "    tokens = [word for word in tokens if word != 'reuters']\n",
    "\n",
    "    # 4. Remove custom stop words (prepositions) and standard stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_prepositions = {'above', 'across', 'after', 'against', 'along', 'among', \n",
    "                           'around', 'at', 'before', 'behind', 'below', 'beneath', \n",
    "                           'beside', 'between', 'beyond', 'by', 'despite', 'down', \n",
    "                           'except', 'for', 'from', 'in', 'inside', 'into', 'near', \n",
    "                           'of', 'off', 'on', 'onto', 'out', 'over', 'past', \n",
    "                           'regarding', 'round', 'since', 'through', 'throughout', \n",
    "                           'till', 'to', 'toward', 'under', 'underneath', 'until', \n",
    "                           'unto', 'up', 'upon', 'with', 'within', 'without'}\n",
    "\n",
    "    custom_stop_words = {  \n",
    "        'china', 'usa', 'us', 'america', 'american', 'americans', 'chinese', 'russia', \n",
    "        'russian', 'putin', 'vladimir', 'trump', 'donald', 'biden', 'joe', 'ukraine', \n",
    "        'ukrainian', 'ukrainians', 'ukraines', 'say', 'jan', 'feb', 'mar', 'apr', 'may', \n",
    "        'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'mon', 'tue', 'wed', 'thu', 'fri', \n",
    "        'sat', 'sun', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', \n",
    "        'sunday', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', \n",
    "        'september', 'october', 'november', 'december', 'today', 'yesterday', 'tomorrow', \n",
    "        'week', 'month', 'year', 'time', 'day', 'weekend', 'morning', 'afternoon', \n",
    "        'evening', 'night', 'news', 'new'\n",
    "    }\n",
    "    stop_words_list = [ 'shanghai','nichola','roger','msci','iran','states','italy','united','china','usa','us','america','american','americans','chinese','china','russia','russian','putin','vladimir','trump','donald','biden','joe','ukraine','ukrainian','ukrainians','ukraines','ukraine','say','jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec','mon','tue','wed','thu','fri','sat','sun','monday','tuesday','wednesday','thursday','friday','saturday','sunday','january','february','march','april','may','june','july','august','september','october','november','december','today','yesterday','tomorrow','week','month','year','time','day','weekend','morning','afternoon','evening','night','news','new','news']\n",
    "    list_numbers = ['eoi','name','houthi','uae','euro','yen','instead','liga','len','nhl','one','two','three','four','five','six','seven','eight','nine','ten','hundred','thousand','million','billion','trillion','first','second','third','eighted','series','hong','kong','new','york','los','angeles','san','francisco','las','vegas','san','diego','san','jose']\n",
    "    country_list = ['myanmar','robert','lebanon','iivi','william','zalando','olympic','country','world','africa','China', 'United States of America', 'United Kingdom', 'France', 'Germany', 'Japan', 'Russia', 'Australia', 'Canada', 'India', 'Brazil', 'Italy', 'Spain', 'South Korea', 'Mexico', 'Netherlands', 'Switzerland', 'Sweden', 'Norway', 'Denmark', 'Finland', 'Greece', 'Ireland', 'Portugal', 'Poland', 'Ukraine', 'Romania', 'Belgium', 'Austria', 'Turkey', 'Saudi Arabia', 'United Arab Emirates', 'Iran', 'Iraq', 'Israel', 'Egypt', 'South Africa', 'Argentina', 'Venezuela', 'Thailand', 'Malaysia', 'Singapore', 'Indonesia', 'Philippines', 'Pakistan', 'Bangladesh', 'Nigeria', 'Kenya', 'Tanzania', 'Uganda']\n",
    "    list_append = [\"whose\", \"german\", \"saidsign\", \"ceo\",\"exar\",\"chos\",\"sme\", \"vietnam\", \"gsk\", \"mori\", \"queen\", \"threeyear\",\"would\", \"come\", \"also\", \"could\", \"edit\", \"include\",\"pitch\", \"Britain\", \"Indian\", \"collin\", \"koo\", \"skorea\", \"men\", \"koo\", \"hub\",\"bbva\", \"korea\", \"inc\", \"btp\", \"ntpcs\", \"telecom\", \"omi\",\"jen\",\"andre\", \"spac\", \"sabadell\",\"faa\", \"unicredit\", \"city\", \"georgia\", \"puma\", \"philip\", \"england\", \"tokyo\", \"announce\", \"safrica\", \"andrea\"]\n",
    "    stop_words.update(stop_words_list)\n",
    "    stop_words.update(list_numbers)\n",
    "    stop_words.update(country_list)\n",
    "    stop_words.update(list_append)\n",
    "    stop_words.update(custom_prepositions)\n",
    "    stop_words.update(custom_stop_words)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 5. Remove words longer than 10 letters\n",
    "    tokens = [word for word in tokens if len(word) <= 10]\n",
    "\n",
    "    # 6. POS tagging and lemmatization for accurate tense restoration\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag:\n",
    "            lemmatized = lemmatizer.lemmatize(word, wn_tag)\n",
    "        else:\n",
    "            lemmatized = lemmatizer.lemmatize(word)\n",
    "        lemmatized_tokens.append(lemmatized)\n",
    "\n",
    "    # 7. Remove specific words (performed after lemmatization)\n",
    "    words_to_remove = {'licensing', 'right', 'thomson', 'trust', 'tabsuggested', \n",
    "                   'principle', 'open', 'new', 'standard', 'say', 'co', 'ltd'}\n",
    "    words_to_remove.update(['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', \n",
    "                        'nine', 'ten', 'hundred', 'thousand', 'million', 'billion', 'trillion'])\n",
    "\n",
    "    lemmatized_tokens = [word for word in lemmatized_tokens if word not in words_to_remove]\n",
    "\n",
    "\n",
    "\n",
    "    #8. Remove single-letter words\n",
    "    lemmatized_tokens = [word for word in lemmatized_tokens if len(word) >= 2]\n",
    "\n",
    "    return lemmatized_tokens\n",
    "    \n",
    "def process_articles(input_directory, output_directory):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    articles = []  # List to store processed articles\n",
    "\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    article = json.load(f)\n",
    "\n",
    "                # Check if the article is in English\n",
    "                if not article.get('title') or not article.get('body'):\n",
    "                    continue\n",
    "\n",
    "                # Preprocess the title and body\n",
    "                article['title'] = preprocess_text(article['title'])\n",
    "                article['body'] = preprocess_text(article['body'])\n",
    "\n",
    "                # Check if the article is empty\n",
    "                if not article['title'] or not article['body']:\n",
    "                    continue\n",
    "\n",
    "                # Check for duplicates\n",
    "                if article['title'] in [a['title'] for a in articles]:\n",
    "                    continue\n",
    "\n",
    "                # Add the date field\n",
    "                article['date'] = article.get('date', 'Unknown')  # Use 'Unknown' if date is missing\n",
    "\n",
    "                articles.append(article)  # Add the processed article to the list\n",
    "\n",
    "    # Convert the list of articles to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # New step: Convert the body field's token list into a space-delimited string\n",
    "    df['body'] = df['body'].apply(lambda x: ', '.join(x))  # Key modification: space -> comma\n",
    "    df['title'] = df['title'].apply(lambda x: ', '.join(x))  # The title needs to be processed as well\n",
    "\n",
    "    # Keep only the title, date, and body columns\n",
    "    df = df[['title', 'date', 'body']]\n",
    "\n",
    "    # Build the output file path\n",
    "    output_path = os.path.join(output_directory, \"processed_articles.csv\")\n",
    "\n",
    "    # Save the processed data to a CSV file\n",
    "    df.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)  \n",
    "\n",
    "    print(f\"Processed data saved to CSV file: {output_path}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = r'D:\\data\\2021'  # Path to the directory containing the JSON files\n",
    "    output_directory = r'D:\\data\\data2021'  # Path to the output directory\n",
    "    process_articles(input_directory, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
