{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e640d59-861a-487a-b314-1ed9c227e19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca00a71f-9271-4ff9-bea5-c5cb27874bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')       # Tokenization\n",
    "nltk.download('stopwords')   # Stop words\n",
    "nltk.download('wordnet')     # Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f93780f-95a9-4dab-a6f1-35efe047f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the NLTK_DATA environment variable to point to your NLTK data directory\n",
    "os.environ['NLTK_DATA'] = \"/path/to/nltk_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14258d90-777f-452d-8384-5cb586e6f6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/maojialu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to CSV file: processed_articles/processed_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map Treebank POS tags to WordNet POS tags for accurate lemmatization.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Clean the text: remove HTML tags, special characters, and numbers\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
    "\n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "\n",
    "    # 3. Remove 'reuters' tokens (case-insensitive)\n",
    "    tokens = [word for word in tokens if word != 'reuters']\n",
    "\n",
    "    # 4. Remove single-letter words\n",
    "    tokens = [word for word in tokens if len(word) >= 2]\n",
    "\n",
    "    # 5. Remove custom stop words (prepositions) and standard stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_prepositions = {'above', 'across', 'after', 'against', 'along', 'among', \n",
    "                           'around', 'at', 'before', 'behind', 'below', 'beneath', \n",
    "                           'beside', 'between', 'beyond', 'by', 'despite', 'down', \n",
    "                           'during', 'except', 'for', 'from', 'in', 'inside', 'into', \n",
    "                           'near', 'of', 'off', 'on', 'onto', 'out', 'over', 'past', \n",
    "                           'regarding', 'round', 'since', 'through', 'throughout', \n",
    "                           'till', 'to', 'toward', 'under', 'underneath', 'until', \n",
    "                           'unto', 'up', 'upon', 'with', 'within', 'without'}\n",
    "    stop_words.update(custom_prepositions)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 6. Remove the specified word list\n",
    "    words_to_remove = {'licensing', 'right', 'thomson', 'trust', 'tabsuggested', 'principle', 'open', 'new', 'standard'}\n",
    "    tokens = [word for word in tokens if word not in words_to_remove]\n",
    "\n",
    "    # 7. Remove words longer than 10 letters\n",
    "    tokens = [word for word in tokens if len(word) <= 10]\n",
    "\n",
    "    # 8. POS tagging and lemmatization for accurate tense restoration\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag:\n",
    "            lemmatized = lemmatizer.lemmatize(word, wn_tag)\n",
    "        else:\n",
    "            lemmatized = lemmatizer.lemmatize(word)\n",
    "        lemmatized_tokens.append(lemmatized)\n",
    "\n",
    "    return lemmatized_tokens  # Return the list of tokens\n",
    "\n",
    "def process_articles(input_directory, output_directory):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    articles = []  # List to store processed articles\n",
    "\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    article = json.load(f)\n",
    "\n",
    "                # Check if the article is in English\n",
    "                if not article.get('title') or not article.get('body'):\n",
    "                    continue\n",
    "\n",
    "                # Preprocess the title and body\n",
    "                article['title'] = preprocess_text(article['title'])\n",
    "                article['body'] = preprocess_text(article['body'])\n",
    "\n",
    "                # Check if the article is empty\n",
    "                if not article['title'] or not article['body']:\n",
    "                    continue\n",
    "\n",
    "                # Check for duplicates\n",
    "                if article['title'] in [a['title'] for a in articles]:\n",
    "                    continue\n",
    "\n",
    "                # Add the date field\n",
    "                article['date'] = article.get('date', 'Unknown')  # Use 'Unknown' if date is missing\n",
    "\n",
    "                articles.append(article)  # Add the processed article to the list\n",
    "\n",
    "    # Convert the list of articles to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Keep only the title, date, and body columns\n",
    "    df = df[['title', 'date', 'body']]\n",
    "\n",
    "    # Build the output file path\n",
    "    output_path = os.path.join(output_directory, \"processed_articles.csv\")\n",
    "\n",
    "    # Save the processed data to a CSV file\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Processed data saved to CSV file: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"articles\"  # Path to the directory containing the JSON files\n",
    "    output_directory = \"processed_articles\"  # Path to the output directory\n",
    "    process_articles(input_directory, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
