{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dcf73c-5fc6-478b-96d7-58e1547c0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv \n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map Treebank POS tags to WordNet POS tags for accurate lemmatization.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def preprocess_text(text):\n",
    "    # 1. Clean the text: remove HTML tags, special characters, and numbers\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 2. Tokenization\n",
    "    tokenizer = RegexpTokenizer(r'\\b[a-zA-Z-]{2,}\\b')  # Only keep words with â‰¥ 2 letters\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    # 3. Remove 'reuters' tokens (case-insensitive)\n",
    "    tokens = [word for word in tokens if word != 'reuters']\n",
    "\n",
    "    # 4. Remove custom stop words (prepositions) and standard stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_prepositions = {'above', 'across', 'after', 'against', 'along', 'among', \n",
    "                           'around', 'at', 'before', 'behind', 'below', 'beneath', \n",
    "                           'beside', 'between', 'beyond', 'by', 'despite', 'down', \n",
    "                           'except', 'for', 'from', 'in', 'inside', 'into', 'near', \n",
    "                           'of', 'off', 'on', 'onto', 'out', 'over', 'past', \n",
    "                           'regarding', 'round', 'since', 'through', 'throughout', \n",
    "                           'till', 'to', 'toward', 'under', 'underneath', 'until', \n",
    "                           'unto', 'up', 'upon', 'with', 'within', 'without'}\n",
    "\n",
    "    custom_stop_words = {  \n",
    "        'china', 'usa', 'us', 'america', 'american', 'americans', 'chinese', 'russia', \n",
    "        'russian', 'putin', 'vladimir', 'trump', 'donald', 'biden', 'joe', 'ukraine', \n",
    "        'ukrainian', 'ukrainians', 'ukraines', 'say', 'jan', 'feb', 'mar', 'apr', 'may', \n",
    "        'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'mon', 'tue', 'wed', 'thu', 'fri', \n",
    "        'sat', 'sun', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', \n",
    "        'sunday', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', \n",
    "        'september', 'october', 'november', 'december', 'today', 'yesterday', 'tomorrow', \n",
    "        'week', 'month', 'year', 'time', 'day', 'weekend', 'morning', 'afternoon', \n",
    "        'evening', 'night', 'news', 'new'\n",
    "    }\n",
    "\n",
    "    stop_words_list = { 'shanghai','nichola','roger','msci','iran','states','italy','united',\n",
    "        'china','usa','us','america','american','americans','chinese','china','russia',\n",
    "        'russian','putin','vladimir','trump','donald','biden','joe','ukraine','ukrainian',\n",
    "        'ukrainians','ukraines','ukraine','say','jan','feb','mar','apr','may','jun','jul',\n",
    "        'aug','sep','oct','nov','dec','mon','tue','wed','thu','fri','sat','sun','monday',\n",
    "        'tuesday','wednesday','thursday','friday','saturday','sunday','january','february',\n",
    "        'march','april','may','june','july','august','september','october','november',\n",
    "        'december','today','yesterday','tomorrow','week','month','year','time','day','weekend',\n",
    "        'morning','afternoon','evening','night','news','new','news'}\n",
    "\n",
    "    list_numbers = {'eoi','name','houthi','uae','euro','yen','instead','liga','len','nhl','one','two',\n",
    "        'three','four','five','six','seven','eight','nine','ten','hundred','thousand','million',\n",
    "        'billion','trillion','first','second','third','eighted','series','hong','kong','new','york',\n",
    "        'los','angeles','san','francisco','las','vegas','san','diego','san','jose'}\n",
    "\n",
    "    country_list = {'myanmar','robert','lebanon','iivi','william','zalando','olympic','country',\n",
    "        'world','africa','China', 'United States of America', 'United Kingdom', 'France', 'Germany',\n",
    "         'Japan', 'Russia', 'Australia', 'Canada', 'India', 'Brazil', 'Italy', 'Spain', 'South Korea',\n",
    "          'Mexico', 'Netherlands', 'Switzerland', 'Sweden', 'Norway', 'Denmark', 'Finland', 'Greece',\n",
    "           'Ireland', 'Portugal', 'Poland', 'Ukraine', 'Romania', 'Belgium', 'Austria', 'Turkey', \n",
    "           'Saudi Arabia', 'United Arab Emirates', 'Iran', 'Iraq', 'Israel', 'Egypt', 'South Africa', \n",
    "           'Argentina', 'Venezuela', 'Thailand', 'Malaysia', 'Singapore', 'Indonesia', 'Philippines',\n",
    "            'Pakistan', 'Bangladesh', 'Nigeria', 'Kenya', 'Tanzania', 'Uganda'}\n",
    "\n",
    "    list_append = {\"whose\", \"german\", \"saidsign\", \"ceo\",\"exar\",\"chos\",\"sme\", \"vietnam\", \"gsk\", \"mori\", \n",
    "        \"queen\", \"threeyear\",\"would\", \"come\", \"also\", \"could\", \"edit\", \"include\",\"pitch\", \"Britain\", \"Indian\", \n",
    "        \"collin\", \"koo\", \"skorea\", \"men\", \"koo\", \"hub\",\"bbva\", \"korea\", \"inc\", \"btp\", \"ntpcs\", \"telecom\", \"omi\",\n",
    "        \"jen\",\"andre\", \"spac\", \"sabadell\",\"faa\", \"unicredit\", \"city\", \"georgia\", \"puma\", \"philip\", \"england\",\n",
    "        \"tokyo\", \"announce\", \"safrica\", \"andrea\"}\n",
    "    \n",
    "    stop_words.update(custom_prepositions)\n",
    "    stop_words.update(custom_stop_words)\n",
    "    stop_words.update(stop_words_list)\n",
    "    stop_words.update(list_numbers)\n",
    "    stop_words.update(country_list)\n",
    "    stop_words.update(list_append)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 5. Remove words longer than 10 letters\n",
    "    tokens = [word for word in tokens if len(word) <= 10]\n",
    "\n",
    "    # 6. POS tagging and lemmatization for accurate tense restoration\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag:\n",
    "            lemmatized = lemmatizer.lemmatize(word, wn_tag)\n",
    "        else:\n",
    "            lemmatized = lemmatizer.lemmatize(word)\n",
    "        lemmatized_tokens.append(lemmatized)\n",
    "\n",
    "    # 7. Remove specific words (performed after lemmatization)\n",
    "    words_to_remove = {'licensing', 'right', 'thomson', 'trust', 'tabsuggested', \n",
    "                   'principle', 'open', 'new', 'standard', 'say', 'co', 'ltd'}\n",
    "    words_to_remove.update(['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', \n",
    "                        'nine', 'ten', 'hundred', 'thousand', 'million', 'billion', 'trillion'])\n",
    "\n",
    "    lemmatized_tokens = [word for word in lemmatized_tokens if word not in words_to_remove]\n",
    "\n",
    "\n",
    "\n",
    "    #8. Remove single-letter words\n",
    "    lemmatized_tokens = [word for word in lemmatized_tokens if len(word) >= 2]\n",
    "\n",
    "    return lemmatized_tokens    \n",
    "def process_articles(input_directory, output_directory):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    articles = []  # List to store processed articles\n",
    "\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    article = json.load(f)\n",
    "\n",
    "                # Check if the article is in English\n",
    "                if not article.get('title') or not article.get('body'):\n",
    "                    continue\n",
    "\n",
    "                # Preprocess the title and body\n",
    "                article['title'] = preprocess_text(article['title'])\n",
    "                article['body'] = preprocess_text(article['body'])\n",
    "\n",
    "                # Check if the article is empty\n",
    "                if not article['title'] or not article['body']:\n",
    "                    continue\n",
    "\n",
    "                # Check for duplicates\n",
    "                if article['title'] in [a['title'] for a in articles]:\n",
    "                    continue\n",
    "\n",
    "                # Add the date field\n",
    "                article['date'] = article.get('date', 'Unknown')  # Use 'Unknown' if date is missing\n",
    "\n",
    "                articles.append(article)  # Add the processed article to the list\n",
    "\n",
    "    # Convert the list of articles to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # New step: Convert the body field's token list into a space-delimited string\n",
    "    df['body'] = df['body'].apply(lambda x: ', '.join(x))  # Key modification: space -> comma\n",
    "    df['title'] = df['title'].apply(lambda x: ', '.join(x))  # The title needs to be processed as well\n",
    "\n",
    "    # Keep only the title, date, and body columns\n",
    "    df = df[['title', 'date', 'body']]\n",
    "\n",
    "    # Build the output file path\n",
    "    output_path = os.path.join(output_directory, \"processed_articles.csv\")\n",
    "\n",
    "    # Save the processed data to a CSV file\n",
    "    df.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)  \n",
    "\n",
    "    print(f\"Processed data saved to CSV file: {output_path}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"articles\"  # Path to the directory containing the JSON files\n",
    "    output_directory = \"processed_articles\"  # Path to the output directory\n",
    "    process_articles(input_directory, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
