{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markham\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import torch\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "test_data = pd.read_csv('data/processed_articles_2021.csv')\n",
    "add_data = pd.read_csv('data/2021_add.csv')\n",
    "# combine these two datasets\n",
    "test_data = pd.concat([test_data, add_data], axis=0)\n",
    "test_data['content'] = test_data['title'] + test_data['body']\n",
    "test_data = test_data.drop(columns=['title','body'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "test_data = test_data.sort_values(by='date')\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join(x.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_list = ['aviva','company','shenzhen','kate','euros','emirate','dhabi','metre','asia','europe','shanghai','nichola','roger','msci','iran','states','italy','united','china','usa','us','america','american','americans','chinese','china','russia','russian','putin','vladimir','trump','donald','biden','joe','ukraine','ukrainian','ukrainians','ukraines','ukraine','say','jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec','mon','tue','wed','thu','fri','sat','sun','monday','tuesday','wednesday','thursday','friday','saturday','sunday','january','february','march','april','may','june','july','august','september','october','november','december','today','yesterday','tomorrow','week','month','year','time','day','weekend','morning','afternoon','evening','night','news','new','news']\n",
    "list_numbers = ['eoi','name','houthi','uae','euro','yen','instead','liga','len','nhl','one','two','three','four','five','six','seven','eight','nine','ten','hundred','thousand','million','billion','trillion','first','second','third','eighted','series','hong','kong','new','york','los','angeles','san','francisco','las','vegas','san','diego','san','jose']\n",
    "country_list = ['myanmar','robert','lebanon','iivi','william','zalando','olympic','country','world','africa','China', 'United States of America', 'United Kingdom', 'France', 'Germany', 'Japan', 'Russia', 'Australia', 'Canada', 'India', 'Brazil', 'Italy', 'Spain', 'South Korea', 'Mexico', 'Netherlands', 'Switzerland', 'Sweden', 'Norway', 'Denmark', 'Finland', 'Greece', 'Ireland', 'Portugal', 'Poland', 'Ukraine', 'Romania', 'Belgium', 'Austria', 'Turkey', 'Saudi Arabia', 'United Arab Emirates', 'Iran', 'Iraq', 'Israel', 'Egypt', 'South Africa', 'Argentina', 'Venezuela', 'Thailand', 'Malaysia', 'Singapore', 'Indonesia', 'Philippines', 'Pakistan', 'Bangladesh', 'Nigeria', 'Kenya', 'Tanzania', 'Uganda']\n",
    "list_append = [\"whose\", \"german\", \"saidsign\", \"ceo\",\"exar\",\"chos\",\"sme\", \"vietnam\", \"gsk\", \"mori\", \"queen\", \"threeyear\",\"would\", \"come\", \"also\", \"could\", \"edit\", \"include\",\"pitch\", \"Britain\", \"Indian\", \"collin\", \"koo\", \"skorea\", \"men\", \"koo\", \"hub\",\"bbva\", \"korea\", \"inc\", \"btp\", \"ntpcs\", \"telecom\", \"omi\",\"jen\",\"andre\", \"spac\", \"sabadell\",\"faa\", \"unicredit\", \"city\", \"georgia\", \"puma\", \"philip\", \"england\", \"tokyo\", \"announce\", \"safrica\", \"andrea\"]\n",
    "stop_words.update(stop_words_list)\n",
    "stop_words.update(list_numbers)\n",
    "stop_words.update(country_list)\n",
    "stop_words.update(list_append)\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word != 'nan']))\n",
    "test_data['content_tokens'] = test_data['content'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>content_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33732</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>argentina grain inspector continue strike unio...</td>\n",
       "      <td>[argentina, grain, inspector, continue, strike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33666</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>man utds cavani peace accept sanctionmancheste...</td>\n",
       "      <td>[man, utds, cavani, peace, accept, sanctionman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33667</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>bitcoin touch record extend rallyyork price bi...</td>\n",
       "      <td>[bitcoin, touch, record, extend, rallyyork, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33668</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>lock berliner spark blaze home fireworkberlin ...</td>\n",
       "      <td>[lock, berliner, spark, blaze, home, fireworkb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33669</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>northern france truck cross custom border ukca...</td>\n",
       "      <td>[northern, france, truck, cross, custom, borde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34260</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>suspect militant kill soldier northern mali ar...</td>\n",
       "      <td>[suspect, militant, kill, soldier, northern, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34261</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>egypt lending apps boost woman business ownerc...</td>\n",
       "      <td>[egypt, lending, apps, boost, woman, business,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34262</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>brazilian election affect petrobras price pled...</td>\n",
       "      <td>[brazilian, election, affect, petrobras, price...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34253</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>tattoo target seek set good examplehong ban ge...</td>\n",
       "      <td>[tattoo, target, seek, set, good, examplehong,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34197</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>insight woman force change indian iphone plant...</td>\n",
       "      <td>[insight, woman, force, change, indian, iphone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48948 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                            content  \\\n",
       "33732 2021-01-01  argentina grain inspector continue strike unio...   \n",
       "33666 2021-01-01  man utds cavani peace accept sanctionmancheste...   \n",
       "33667 2021-01-01  bitcoin touch record extend rallyyork price bi...   \n",
       "33668 2021-01-01  lock berliner spark blaze home fireworkberlin ...   \n",
       "33669 2021-01-01  northern france truck cross custom border ukca...   \n",
       "...          ...                                                ...   \n",
       "34260 2021-12-30  suspect militant kill soldier northern mali ar...   \n",
       "34261 2021-12-30  egypt lending apps boost woman business ownerc...   \n",
       "34262 2021-12-30  brazilian election affect petrobras price pled...   \n",
       "34253 2021-12-30  tattoo target seek set good examplehong ban ge...   \n",
       "34197 2021-12-30  insight woman force change indian iphone plant...   \n",
       "\n",
       "                                          content_tokens  \n",
       "33732  [argentina, grain, inspector, continue, strike...  \n",
       "33666  [man, utds, cavani, peace, accept, sanctionman...  \n",
       "33667  [bitcoin, touch, record, extend, rallyyork, pr...  \n",
       "33668  [lock, berliner, spark, blaze, home, fireworkb...  \n",
       "33669  [northern, france, truck, cross, custom, borde...  \n",
       "...                                                  ...  \n",
       "34260  [suspect, militant, kill, soldier, northern, m...  \n",
       "34261  [egypt, lending, apps, boost, woman, business,...  \n",
       "34262  [brazilian, election, affect, petrobras, price...  \n",
       "34253  [tattoo, target, seek, set, good, examplehong,...  \n",
       "34197  [insight, woman, force, change, indian, iphone...  \n",
       "\n",
       "[48948 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the date with less than 60 samples, output the data which can be used for the recollection\n",
    "start_date = '2021-01-01'\n",
    "end_date = '2021-12-31'\n",
    "test_data = test_data[(test_data['date'] >= start_date) & (test_data['date'] <= end_date)]\n",
    "test_data\n",
    "sample_counts = test_data['date'].value_counts()\n",
    "\n",
    "# find the dates with more than 60 samples and keep them as the test data\n",
    "valid_dates = sample_counts[sample_counts >= 30].index\n",
    "test_data = test_data[test_data['date'].isin(valid_dates)]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Silhouette parameters: Vector Size=70, Window=7, Min Count=2, N=20\n",
      "Best Calinski-Harabasz parameters: Vector Size=70, Window=7, Min Count=3, N=20\n"
     ]
    }
   ],
   "source": [
    "# Define parameter search space\n",
    "vector_sizes = [70, 80, 90]\n",
    "windows = [ 6,7, 8]\n",
    "min_counts = [2,3]  \n",
    "Ns = [10,15,20]\n",
    "\n",
    "best_silhouette_params = None\n",
    "best_silhouette_score = -1\n",
    "\n",
    "best_ch_params = None\n",
    "best_ch_score = -1\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window in windows:\n",
    "        for min_count in min_counts:\n",
    "            for N in Ns:\n",
    "                model = Word2Vec(sentences=test_data['content_tokens'], vector_size=vector_size, window=window, min_count=min_count, workers=8)\n",
    "                \n",
    "                corpus = test_data['content'].tolist()\n",
    "                vectorizer = TfidfVectorizer()\n",
    "                tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "                word2tfidf = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "            \n",
    "                test_data['important_words'] = test_data['content_tokens'].apply(\n",
    "                    lambda tokens: sorted(\n",
    "                        set(tokens),  # Remove duplicates\n",
    "                        key=lambda x: word2tfidf.get(x, 0),  # Rank by TF-IDF score\n",
    "                        reverse=False  # Lower TF-IDF means higher frequency in this case\n",
    "                    )[:N]\n",
    "                )\n",
    "                \n",
    "                def text_to_vector(text, model, word2tfidf):\n",
    "                    vectors = []\n",
    "                    weights = []\n",
    "                    for word in text:\n",
    "                        if word in model.wv:\n",
    "                            vectors.append(model.wv[word])\n",
    "                            weights.append(word2tfidf.get(word, 1.0))\n",
    "                    if not vectors:\n",
    "                        return np.zeros(model.vector_size)\n",
    "                    vectors = np.array(vectors)\n",
    "                    weights = np.array(weights) / sum(weights)\n",
    "                    return np.average(vectors, axis=0, weights=weights)\n",
    "                \n",
    "                test_data['vector'] = test_data['important_words'].apply(lambda x: text_to_vector(x, model, word2tfidf))\n",
    "                \n",
    "                X = np.array(test_data['vector'].tolist())\n",
    "                num_clusters = 2\n",
    "                kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "                labels = kmeans.fit_predict(X)\n",
    "                test_data['cluster_label'] = labels\n",
    "                \n",
    "                if len(set(labels)) > 1:\n",
    "                    silhouette = silhouette_score(X, labels)\n",
    "                    ch_score = calinski_harabasz_score(X, labels)\n",
    "                    \n",
    "                    if silhouette > best_silhouette_score: \n",
    "                        best_silhouette_score = silhouette\n",
    "                        best_silhouette_params = (vector_size, window, min_count, N)\n",
    "                    \n",
    "                    if ch_score > best_ch_score:\n",
    "                        best_ch_score = ch_score\n",
    "                        best_ch_params = (vector_size, window, min_count, N)\n",
    "                        \n",
    "print(f\"Best Silhouette parameters: Vector Size={best_silhouette_params[0]}, Window={best_silhouette_params[1]}, Min Count={best_silhouette_params[2]}, N={best_silhouette_params[3]}\")\n",
    "print(f\"Best Calinski-Harabasz parameters: Vector Size={best_ch_params[0]}, Window={best_ch_params[1]}, Min Count={best_ch_params[2]}, N={best_ch_params[3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
