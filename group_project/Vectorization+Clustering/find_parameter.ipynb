{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markham\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import torch\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "test_data = pd.read_csv('data/processed_articles_202101-08.csv')\n",
    "test_data['content'] = test_data['title'] + test_data['body']\n",
    "test_data = test_data.drop(columns=['title','body'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "test_data = test_data.sort_values(by='date')\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join(x.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_list = ['aviva','company','shenzhen','kate','euros','emirate','dhabi','metre','asia','europe','shanghai','nichola','roger','msci','iran','states','italy','united','china','usa','us','america','american','americans','chinese','china','russia','russian','putin','vladimir','trump','donald','biden','joe','ukraine','ukrainian','ukrainians','ukraines','ukraine','say','jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec','mon','tue','wed','thu','fri','sat','sun','monday','tuesday','wednesday','thursday','friday','saturday','sunday','january','february','march','april','may','june','july','august','september','october','november','december','today','yesterday','tomorrow','week','month','year','time','day','weekend','morning','afternoon','evening','night','news','new','news']\n",
    "list_numbers = ['eoi','name','houthi','uae','euro','yen','instead','liga','len','nhl','one','two','three','four','five','six','seven','eight','nine','ten','hundred','thousand','million','billion','trillion','first','second','third','eighted','series','hong','kong','new','york','los','angeles','san','francisco','las','vegas','san','diego','san','jose']\n",
    "country_list = ['myanmar','robert','lebanon','iivi','william','zalando','olympic','country','world','africa','China', 'United States of America', 'United Kingdom', 'France', 'Germany', 'Japan', 'Russia', 'Australia', 'Canada', 'India', 'Brazil', 'Italy', 'Spain', 'South Korea', 'Mexico', 'Netherlands', 'Switzerland', 'Sweden', 'Norway', 'Denmark', 'Finland', 'Greece', 'Ireland', 'Portugal', 'Poland', 'Ukraine', 'Romania', 'Belgium', 'Austria', 'Turkey', 'Saudi Arabia', 'United Arab Emirates', 'Iran', 'Iraq', 'Israel', 'Egypt', 'South Africa', 'Argentina', 'Venezuela', 'Thailand', 'Malaysia', 'Singapore', 'Indonesia', 'Philippines', 'Pakistan', 'Bangladesh', 'Nigeria', 'Kenya', 'Tanzania', 'Uganda']\n",
    "list_append = [\"whose\", \"german\", \"saidsign\", \"ceo\",\"exar\",\"chos\",\"sme\", \"vietnam\", \"gsk\", \"mori\", \"queen\", \"threeyear\",\"would\", \"come\", \"also\", \"could\", \"edit\", \"include\",\"pitch\", \"Britain\", \"Indian\", \"collin\", \"koo\", \"skorea\", \"men\", \"koo\", \"hub\",\"bbva\", \"korea\", \"inc\", \"btp\", \"ntpcs\", \"telecom\", \"omi\",\"jen\",\"andre\", \"spac\", \"sabadell\",\"faa\", \"unicredit\", \"city\", \"georgia\", \"puma\", \"philip\", \"england\", \"tokyo\", \"announce\", \"safrica\", \"andrea\"]\n",
    "stop_words.update(stop_words_list)\n",
    "stop_words.update(list_numbers)\n",
    "stop_words.update(country_list)\n",
    "stop_words.update(list_append)\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word != 'nan']))\n",
    "test_data['content_tokens'] = test_data['content'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>content_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26004</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>exclusive drugmakers hike price pandemic polit...</td>\n",
       "      <td>[exclusive, drugmakers, hike, price, pandemic,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26031</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>australias batsman must grind foil india planm...</td>\n",
       "      <td>[australias, batsman, must, grind, foil, india...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26030</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>saint clear end isolation ahead liverpool clas...</td>\n",
       "      <td>[saint, clear, end, isolation, ahead, liverpoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26029</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>infectious covid variant find florida state of...</td>\n",
       "      <td>[infectious, covid, variant, find, florida, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26028</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>irish covid spread bad formal reporting sugges...</td>\n",
       "      <td>[irish, covid, spread, bad, formal, reporting,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9132</th>\n",
       "      <td>2021-07-31</td>\n",
       "      <td>birthday boy jung take eventing leadtokyo germ...</td>\n",
       "      <td>[birthday, boy, jung, take, eventing, leadtoky...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129</th>\n",
       "      <td>2021-07-31</td>\n",
       "      <td>brazil bolsonaro oil petrobras pay free lpgbra...</td>\n",
       "      <td>[brazil, bolsonaro, oil, petrobras, pay, free,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9128</th>\n",
       "      <td>2021-07-31</td>\n",
       "      <td>game improve prisonlike athlete quarantinetoky...</td>\n",
       "      <td>[game, improve, prisonlike, athlete, quarantin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9127</th>\n",
       "      <td>2021-07-31</td>\n",
       "      <td>tennisno medal injure djokovic straight gamepu...</td>\n",
       "      <td>[tennisno, medal, injure, djokovic, straight, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>2021-07-31</td>\n",
       "      <td>high hope dash japan shuttlers reflect letdown...</td>\n",
       "      <td>[high, hope, dash, japan, shuttlers, reflect, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29646 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                            content  \\\n",
       "26004 2021-01-01  exclusive drugmakers hike price pandemic polit...   \n",
       "26031 2021-01-01  australias batsman must grind foil india planm...   \n",
       "26030 2021-01-01  saint clear end isolation ahead liverpool clas...   \n",
       "26029 2021-01-01  infectious covid variant find florida state of...   \n",
       "26028 2021-01-01  irish covid spread bad formal reporting sugges...   \n",
       "...          ...                                                ...   \n",
       "9132  2021-07-31  birthday boy jung take eventing leadtokyo germ...   \n",
       "9129  2021-07-31  brazil bolsonaro oil petrobras pay free lpgbra...   \n",
       "9128  2021-07-31  game improve prisonlike athlete quarantinetoky...   \n",
       "9127  2021-07-31  tennisno medal injure djokovic straight gamepu...   \n",
       "9126  2021-07-31  high hope dash japan shuttlers reflect letdown...   \n",
       "\n",
       "                                          content_tokens  \n",
       "26004  [exclusive, drugmakers, hike, price, pandemic,...  \n",
       "26031  [australias, batsman, must, grind, foil, india...  \n",
       "26030  [saint, clear, end, isolation, ahead, liverpoo...  \n",
       "26029  [infectious, covid, variant, find, florida, st...  \n",
       "26028  [irish, covid, spread, bad, formal, reporting,...  \n",
       "...                                                  ...  \n",
       "9132   [birthday, boy, jung, take, eventing, leadtoky...  \n",
       "9129   [brazil, bolsonaro, oil, petrobras, pay, free,...  \n",
       "9128   [game, improve, prisonlike, athlete, quarantin...  \n",
       "9127   [tennisno, medal, injure, djokovic, straight, ...  \n",
       "9126   [high, hope, dash, japan, shuttlers, reflect, ...  \n",
       "\n",
       "[29646 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the date with less than 60 samples, output the data which can be used for the recollection\n",
    "start_date = '2021-01-01'\n",
    "end_date = '2021-07-31'\n",
    "test_data = test_data[(test_data['date'] >= start_date) & (test_data['date'] <= end_date)]\n",
    "test_data\n",
    "sample_counts = test_data['date'].value_counts()\n",
    "\n",
    "# find the dates with more than 60 samples and keep them as the test data\n",
    "valid_dates = sample_counts[sample_counts >= 60].index\n",
    "test_data = test_data[test_data['date'].isin(valid_dates)]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Silhouette parameters: Vector Size=80, Window=9, Min Count=2, N=20\n",
      "Best Calinski-Harabasz parameters: Vector Size=70, Window=8, Min Count=3, N=20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define parameter search space\n",
    "vector_sizes = [70, 80, 90]\n",
    "windows = [ 8, 9, 10]\n",
    "min_counts = [2, 3]  \n",
    "Ns = [10,20]\n",
    "\n",
    "best_silhouette_params = None\n",
    "best_silhouette_score = -1\n",
    "\n",
    "best_ch_params = None\n",
    "best_ch_score = -1\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window in windows:\n",
    "        for min_count in min_counts:\n",
    "            for N in Ns:\n",
    "                model = Word2Vec(sentences=test_data['content_tokens'], vector_size=vector_size, window=window, min_count=min_count, workers=8)\n",
    "                \n",
    "                corpus = test_data['content'].tolist()\n",
    "                vectorizer = TfidfVectorizer()\n",
    "                tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "                word2tfidf = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "            \n",
    "                test_data['important_words'] = test_data['content_tokens'].apply(\n",
    "                    lambda tokens: sorted(\n",
    "                        set(tokens),  # Remove duplicates\n",
    "                        key=lambda x: word2tfidf.get(x, 0),  # Rank by TF-IDF score\n",
    "                        reverse=False  # Lower TF-IDF means higher frequency in this case\n",
    "                    )[:N]\n",
    "                )\n",
    "                \n",
    "                def text_to_vector(text, model, word2tfidf):\n",
    "                    vectors = []\n",
    "                    weights = []\n",
    "                    for word in text:\n",
    "                        if word in model.wv:\n",
    "                            vectors.append(model.wv[word])\n",
    "                            weights.append(word2tfidf.get(word, 1.0))\n",
    "                    if not vectors:\n",
    "                        return np.zeros(model.vector_size)\n",
    "                    vectors = np.array(vectors)\n",
    "                    weights = np.array(weights) / sum(weights)\n",
    "                    return np.average(vectors, axis=0, weights=weights)\n",
    "                \n",
    "                test_data['vector'] = test_data['important_words'].apply(lambda x: text_to_vector(x, model, word2tfidf))\n",
    "                \n",
    "                X = np.array(test_data['vector'].tolist())\n",
    "                num_clusters = 2\n",
    "                kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "                labels = kmeans.fit_predict(X)\n",
    "                test_data['cluster_label'] = labels\n",
    "                \n",
    "                if len(set(labels)) > 1:\n",
    "                    silhouette = silhouette_score(X, labels)\n",
    "                    ch_score = calinski_harabasz_score(X, labels)\n",
    "                    \n",
    "                    if silhouette > best_silhouette_score: \n",
    "                        best_silhouette_score = silhouette\n",
    "                        best_silhouette_params = (vector_size, window, min_count, N)\n",
    "                    \n",
    "                    if ch_score > best_ch_score:\n",
    "                        best_ch_score = ch_score\n",
    "                        best_ch_params = (vector_size, window, min_count, N)\n",
    "                        \n",
    "print(f\"Best Silhouette parameters: Vector Size={best_silhouette_params[0]}, Window={best_silhouette_params[1]}, Min Count={best_silhouette_params[2]}, N={best_silhouette_params[3]}\")\n",
    "print(f\"Best Calinski-Harabasz parameters: Vector Size={best_ch_params[0]}, Window={best_ch_params[1]}, Min Count={best_ch_params[2]}, N={best_ch_params[3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
