{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markham\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/processed_articles_2021_V1.csv')\n",
    "test_data['content'] = test_data['title'] + test_data['body']\n",
    "test_data = test_data.drop(columns=['title','body'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "test_data = test_data.sort_values(by='date')\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join(x.split(',')))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_list = [ 'msci','iran','states','italy','united','china','usa','us','america','american','americans','chinese','china','russia','russian','putin','vladimir','trump','donald','biden','joe','ukraine','ukrainian','ukrainians','ukraines','ukraine','say','jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec','mon','tue','wed','thu','fri','sat','sun','monday','tuesday','wednesday','thursday','friday','saturday','sunday','january','february','march','april','may','june','july','august','september','october','november','december','today','yesterday','tomorrow','week','month','year','time','day','weekend','morning','afternoon','evening','night','news','new','news']\n",
    "list_numbers = ['liga','len','nhl','one','two','three','four','five','six','seven','eight','nine','ten','hundred','thousand','million','billion','trillion']\n",
    "country_list = ['China', 'United States of America', 'United Kingdom', 'France', 'Germany', 'Japan', 'Russia', 'Australia', 'Canada', 'India', 'Brazil', 'Italy', 'Spain', 'South Korea', 'Mexico', 'Netherlands', 'Switzerland', 'Sweden', 'Norway', 'Denmark', 'Finland', 'Greece', 'Ireland', 'Portugal', 'Poland', 'Ukraine', 'Romania', 'Belgium', 'Austria', 'Turkey', 'Saudi Arabia', 'United Arab Emirates', 'Iran', 'Iraq', 'Israel', 'Egypt', 'South Africa', 'Argentina', 'Venezuela', 'Thailand', 'Malaysia', 'Singapore', 'Indonesia', 'Philippines', 'Pakistan', 'Bangladesh', 'Nigeria', 'Kenya', 'Tanzania', 'Uganda']\n",
    "stop_words.update(stop_words_list)\n",
    "stop_words.update(list_numbers)\n",
    "stop_words.update(country_list)\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word != 'nan']))\n",
    "test_data['content_tokens'] = test_data['content'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_size=50, window=5, min_count=2, N=10, num_clusters=2 -> silhouette_score=0.3949\n",
      "vector_size=50, window=5, min_count=2, N=10, num_clusters=3 -> silhouette_score=0.3898\n",
      "vector_size=50, window=5, min_count=2, N=10, num_clusters=4 -> silhouette_score=0.3501\n",
      "vector_size=50, window=5, min_count=2, N=15, num_clusters=2 -> silhouette_score=0.3975\n",
      "vector_size=50, window=5, min_count=2, N=15, num_clusters=3 -> silhouette_score=0.3871\n",
      "vector_size=50, window=5, min_count=2, N=15, num_clusters=4 -> silhouette_score=0.2671\n",
      "vector_size=50, window=5, min_count=2, N=20, num_clusters=2 -> silhouette_score=0.3999\n",
      "vector_size=50, window=5, min_count=2, N=20, num_clusters=3 -> silhouette_score=0.3871\n",
      "vector_size=50, window=5, min_count=2, N=20, num_clusters=4 -> silhouette_score=0.2699\n",
      "vector_size=50, window=5, min_count=3, N=10, num_clusters=2 -> silhouette_score=0.3838\n",
      "vector_size=50, window=5, min_count=3, N=10, num_clusters=3 -> silhouette_score=0.3771\n",
      "vector_size=50, window=5, min_count=3, N=10, num_clusters=4 -> silhouette_score=0.3518\n",
      "vector_size=50, window=5, min_count=3, N=15, num_clusters=2 -> silhouette_score=0.3864\n",
      "vector_size=50, window=5, min_count=3, N=15, num_clusters=3 -> silhouette_score=0.3763\n",
      "vector_size=50, window=5, min_count=3, N=15, num_clusters=4 -> silhouette_score=0.3697\n",
      "vector_size=50, window=5, min_count=3, N=20, num_clusters=2 -> silhouette_score=0.3866\n",
      "vector_size=50, window=5, min_count=3, N=20, num_clusters=3 -> silhouette_score=0.3725\n",
      "vector_size=50, window=5, min_count=3, N=20, num_clusters=4 -> silhouette_score=0.3831\n",
      "vector_size=50, window=5, min_count=5, N=10, num_clusters=2 -> silhouette_score=0.4217\n",
      "vector_size=50, window=5, min_count=5, N=10, num_clusters=3 -> silhouette_score=0.3527\n",
      "vector_size=50, window=5, min_count=5, N=10, num_clusters=4 -> silhouette_score=0.2768\n",
      "vector_size=50, window=5, min_count=5, N=15, num_clusters=2 -> silhouette_score=0.4077\n",
      "vector_size=50, window=5, min_count=5, N=15, num_clusters=3 -> silhouette_score=0.3625\n",
      "vector_size=50, window=5, min_count=5, N=15, num_clusters=4 -> silhouette_score=0.2683\n",
      "vector_size=50, window=5, min_count=5, N=20, num_clusters=2 -> silhouette_score=0.3933\n",
      "vector_size=50, window=5, min_count=5, N=20, num_clusters=3 -> silhouette_score=0.3655\n",
      "vector_size=50, window=5, min_count=5, N=20, num_clusters=4 -> silhouette_score=0.2656\n",
      "vector_size=50, window=8, min_count=2, N=10, num_clusters=2 -> silhouette_score=0.3961\n",
      "vector_size=50, window=8, min_count=2, N=10, num_clusters=3 -> silhouette_score=0.3783\n",
      "vector_size=50, window=8, min_count=2, N=10, num_clusters=4 -> silhouette_score=0.2582\n",
      "vector_size=50, window=8, min_count=2, N=15, num_clusters=2 -> silhouette_score=0.4072\n",
      "vector_size=50, window=8, min_count=2, N=15, num_clusters=3 -> silhouette_score=0.3884\n",
      "vector_size=50, window=8, min_count=2, N=15, num_clusters=4 -> silhouette_score=0.2891\n",
      "vector_size=50, window=8, min_count=2, N=20, num_clusters=2 -> silhouette_score=0.4168\n",
      "vector_size=50, window=8, min_count=2, N=20, num_clusters=3 -> silhouette_score=0.3907\n",
      "vector_size=50, window=8, min_count=2, N=20, num_clusters=4 -> silhouette_score=0.2629\n",
      "vector_size=50, window=8, min_count=3, N=10, num_clusters=2 -> silhouette_score=0.4024\n",
      "vector_size=50, window=8, min_count=3, N=10, num_clusters=3 -> silhouette_score=0.3630\n",
      "vector_size=50, window=8, min_count=3, N=10, num_clusters=4 -> silhouette_score=0.3470\n",
      "vector_size=50, window=8, min_count=3, N=15, num_clusters=2 -> silhouette_score=0.4014\n",
      "vector_size=50, window=8, min_count=3, N=15, num_clusters=3 -> silhouette_score=0.3723\n",
      "vector_size=50, window=8, min_count=3, N=15, num_clusters=4 -> silhouette_score=0.3639\n",
      "vector_size=50, window=8, min_count=3, N=20, num_clusters=2 -> silhouette_score=0.3976\n",
      "vector_size=50, window=8, min_count=3, N=20, num_clusters=3 -> silhouette_score=0.3719\n",
      "vector_size=50, window=8, min_count=3, N=20, num_clusters=4 -> silhouette_score=0.3779\n",
      "vector_size=50, window=8, min_count=5, N=10, num_clusters=2 -> silhouette_score=0.3822\n",
      "vector_size=50, window=8, min_count=5, N=10, num_clusters=3 -> silhouette_score=0.3543\n",
      "vector_size=50, window=8, min_count=5, N=10, num_clusters=4 -> silhouette_score=0.2385\n",
      "vector_size=50, window=8, min_count=5, N=15, num_clusters=2 -> silhouette_score=0.3871\n",
      "vector_size=50, window=8, min_count=5, N=15, num_clusters=3 -> silhouette_score=0.3648\n",
      "vector_size=50, window=8, min_count=5, N=15, num_clusters=4 -> silhouette_score=0.2514\n",
      "vector_size=50, window=8, min_count=5, N=20, num_clusters=2 -> silhouette_score=0.3888\n",
      "vector_size=50, window=8, min_count=5, N=20, num_clusters=3 -> silhouette_score=0.3718\n",
      "vector_size=50, window=8, min_count=5, N=20, num_clusters=4 -> silhouette_score=0.3667\n",
      "vector_size=50, window=10, min_count=2, N=10, num_clusters=2 -> silhouette_score=0.3974\n",
      "vector_size=50, window=10, min_count=2, N=10, num_clusters=3 -> silhouette_score=0.3588\n",
      "vector_size=50, window=10, min_count=2, N=10, num_clusters=4 -> silhouette_score=0.2386\n",
      "vector_size=50, window=10, min_count=2, N=15, num_clusters=2 -> silhouette_score=0.4062\n",
      "vector_size=50, window=10, min_count=2, N=15, num_clusters=3 -> silhouette_score=0.3763\n",
      "vector_size=50, window=10, min_count=2, N=15, num_clusters=4 -> silhouette_score=0.2519\n",
      "vector_size=50, window=10, min_count=2, N=20, num_clusters=2 -> silhouette_score=0.4140\n",
      "vector_size=50, window=10, min_count=2, N=20, num_clusters=3 -> silhouette_score=0.3802\n",
      "vector_size=50, window=10, min_count=2, N=20, num_clusters=4 -> silhouette_score=0.2629\n"
     ]
    }
   ],
   "source": [
    "# 参数搜索范围\n",
    "vector_sizes = [50, 60,70, 80]  # 词向量维度\n",
    "windows = [5, 8, 10]  # 窗口大小\n",
    "min_counts = [2, 3, 5]  # 最小出现次数\n",
    "num_clusters_list = [2,3,4] # K-Means 聚类个数\n",
    "N_values = [ 10, 15, 20]  # 选择最重要的 N 个词\n",
    "\n",
    "best_score = -1\n",
    "best_params = {}\n",
    "\n",
    "# 遍历所有参数组合\n",
    "for vector_size in vector_sizes:\n",
    "    for window in windows:\n",
    "        for min_count in min_counts:\n",
    "            # 训练 Word2Vec 模型\n",
    "            model = Word2Vec(sentences=test_data['content_tokens'], \n",
    "                             vector_size=vector_size, window=window, \n",
    "                             min_count=min_count, workers=8)\n",
    "            \n",
    "            # 初始化 VADER 分析器\n",
    "            sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "            # 获取积极和消极词\n",
    "            positive_seed_words = [word for word, score in sid.lexicon.items() if score > 1.0 and word.isalpha()]\n",
    "            negative_seed_words = [word for word, score in sid.lexicon.items() if score < -1.0 and word.isalpha()]\n",
    "\n",
    "            # 过滤不在 Word2Vec 词汇表中的种子词\n",
    "            positive_seed_words = [word for word in positive_seed_words if word in model.wv]\n",
    "            negative_seed_words = [word for word in negative_seed_words if word in model.wv]\n",
    "\n",
    "            # 计算种子词相似度\n",
    "            word_similarities = {}\n",
    "            for word in model.wv.index_to_key:\n",
    "                pos_sim = max([model.wv.similarity(word, seed) for seed in positive_seed_words], default=0)\n",
    "                neg_sim = max([model.wv.similarity(word, seed) for seed in negative_seed_words], default=0)\n",
    "                word_similarities[word] = max(pos_sim, neg_sim)\n",
    "\n",
    "            for N in N_values:\n",
    "                # 选择最重要的 N 个词\n",
    "                test_data['important_words'] = test_data['content_tokens'].apply(\n",
    "                    lambda tokens: sorted(\n",
    "                        [token for token in tokens if token in word_similarities],\n",
    "                        key=lambda x: word_similarities[x],\n",
    "                        reverse=True\n",
    "                    )[:N]\n",
    "                )\n",
    "\n",
    "                # 将文本转换为向量\n",
    "                def text_to_vector(text, model):\n",
    "                    vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "                    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "                test_data['vector'] = test_data['important_words'].apply(lambda x: text_to_vector(x, model))\n",
    "\n",
    "                # 进行 K-Means 聚类并计算 Silhouette Score\n",
    "                X = np.array(test_data['vector'].tolist())\n",
    "\n",
    "\n",
    "                for num_clusters in num_clusters_list:\n",
    "                    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "                    labels = kmeans.fit_predict(X)\n",
    "\n",
    "                    score = silhouette_score(X, labels)  # 计算轮廓系数\n",
    "                    print(f\"vector_size={vector_size}, window={window}, min_count={min_count}, N={N}, num_clusters={num_clusters} -> silhouette_score={score:.4f}\")\n",
    "\n",
    "                    # 更新最佳参数\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {\n",
    "                            \"vector_size\": vector_size,\n",
    "                            \"window\": window,\n",
    "                            \"min_count\": min_count,\n",
    "                            \"N\": N,\n",
    "                            \"num_clusters\": num_clusters,\n",
    "                            \"silhouette_score\": score\n",
    "                        }\n",
    "\n",
    "# 输出最佳参数\n",
    "print(\"\\n✅ Best Parameters:\")\n",
    "print(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
