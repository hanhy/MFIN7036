{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markham\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/processed_articles_2021_V1.csv')\n",
    "test_data['content'] = test_data['title'] + test_data['body']\n",
    "test_data = test_data.drop(columns=['title','body'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "test_data = test_data.sort_values(by='date')\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join(x.split(',')))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_list = [ 'msci','iran','states','italy','united','china','usa','us','america','american','americans','chinese','china','russia','russian','putin','vladimir','trump','donald','biden','joe','ukraine','ukrainian','ukrainians','ukraines','ukraine','say','jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec','mon','tue','wed','thu','fri','sat','sun','monday','tuesday','wednesday','thursday','friday','saturday','sunday','january','february','march','april','may','june','july','august','september','october','november','december','today','yesterday','tomorrow','week','month','year','time','day','weekend','morning','afternoon','evening','night','news','new','news']\n",
    "list_numbers = ['liga','len','nhl','one','two','three','four','five','six','seven','eight','nine','ten','hundred','thousand','million','billion','trillion']\n",
    "country_list = ['China', 'United States of America', 'United Kingdom', 'France', 'Germany', 'Japan', 'Russia', 'Australia', 'Canada', 'India', 'Brazil', 'Italy', 'Spain', 'South Korea', 'Mexico', 'Netherlands', 'Switzerland', 'Sweden', 'Norway', 'Denmark', 'Finland', 'Greece', 'Ireland', 'Portugal', 'Poland', 'Ukraine', 'Romania', 'Belgium', 'Austria', 'Turkey', 'Saudi Arabia', 'United Arab Emirates', 'Iran', 'Iraq', 'Israel', 'Egypt', 'South Africa', 'Argentina', 'Venezuela', 'Thailand', 'Malaysia', 'Singapore', 'Indonesia', 'Philippines', 'Pakistan', 'Bangladesh', 'Nigeria', 'Kenya', 'Tanzania', 'Uganda']\n",
    "stop_words.update(stop_words_list)\n",
    "stop_words.update(list_numbers)\n",
    "stop_words.update(country_list)\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word != 'nan']))\n",
    "test_data['content_tokens'] = test_data['content'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Silhouette parameters: Vector Size=100, Window=8, Min Count=2, N=30\n",
      "Best Calinski-Harabasz parameters: Vector Size=100, Window=8, Min Count=2, N=30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "# Define parameter search space\n",
    "vector_sizes = [30, 40, 50, 60, 70, 80, 90, 100]\n",
    "windows = [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "min_counts = [2, 3, 4, 5, 6, 7]\n",
    "Ns = [10, 20, 30, 40, 50]\n",
    "\n",
    "best_silhouette_params = None\n",
    "best_silhouette_score = -1\n",
    "\n",
    "best_ch_params = None\n",
    "best_ch_score = -1\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window in windows:\n",
    "        for min_count in min_counts:\n",
    "            for N in Ns:\n",
    "                # Step 2: Train Word2Vec model on tokenized words\n",
    "                model = Word2Vec(sentences=test_data['content_tokens'], vector_size=vector_size, window=window, min_count=min_count, workers=8)\n",
    "                \n",
    "                # Step 3: Compute TF-IDF weights for all words in the dataset\n",
    "                corpus = test_data['content'].tolist()\n",
    "                vectorizer = TfidfVectorizer()\n",
    "                tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "                word2tfidf = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "                \n",
    "                # Step 4: Identify high-frequency words per article\n",
    "                test_data['important_words'] = test_data['content_tokens'].apply(\n",
    "                    lambda tokens: sorted(\n",
    "                        set(tokens),  # Remove duplicates\n",
    "                        key=lambda x: word2tfidf.get(x, 0),  # Rank by TF-IDF score\n",
    "                        reverse=False  # Lower TF-IDF means higher frequency in this case\n",
    "                    )[:N]\n",
    "                )\n",
    "                \n",
    "                # Step 5: Convert text to weighted vectors\n",
    "                def text_to_vector(text, model, word2tfidf):\n",
    "                    vectors = []\n",
    "                    weights = []\n",
    "                    for word in text:\n",
    "                        if word in model.wv:\n",
    "                            vectors.append(model.wv[word])\n",
    "                            weights.append(word2tfidf.get(word, 1.0))\n",
    "                    if not vectors:\n",
    "                        return np.zeros(model.vector_size)\n",
    "                    vectors = np.array(vectors)\n",
    "                    weights = np.array(weights) / sum(weights)\n",
    "                    return np.average(vectors, axis=0, weights=weights)\n",
    "                \n",
    "                test_data['vector'] = test_data['important_words'].apply(lambda x: text_to_vector(x, model, word2tfidf))\n",
    "                \n",
    "                # Step 6: Prepare data for clustering\n",
    "                X = np.array(test_data['vector'].tolist())\n",
    "                \n",
    "                # Step 7: Perform clustering\n",
    "                num_clusters = 2\n",
    "                kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "                labels = kmeans.fit_predict(X)\n",
    "                test_data['cluster_label'] = labels\n",
    "                \n",
    "                # Step 8: Evaluate clustering\n",
    "                if len(set(labels)) > 1:\n",
    "                    silhouette = silhouette_score(X, labels)\n",
    "                    ch_score = calinski_harabasz_score(X, labels)\n",
    "                    \n",
    "                    if silhouette > best_silhouette_score:\n",
    "                        best_silhouette_score = silhouette\n",
    "                        best_silhouette_params = (vector_size, window, min_count, N)\n",
    "                    \n",
    "                    if ch_score > best_ch_score:\n",
    "                        best_ch_score = ch_score\n",
    "                        best_ch_params = (vector_size, window, min_count, N)\n",
    "                        \n",
    "print(f\"Best Silhouette parameters: Vector Size={best_silhouette_params[0]}, Window={best_silhouette_params[1]}, Min Count={best_silhouette_params[2]}, N={best_silhouette_params[3]}\")\n",
    "print(f\"Best Calinski-Harabasz parameters: Vector Size={best_ch_params[0]}, Window={best_ch_params[1]}, Min Count={best_ch_params[2]}, N={best_ch_params[3]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
