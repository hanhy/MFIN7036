{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/huiyang.han/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import torch\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "test_data = pd.read_csv('data/processed_articles.csv')\n",
    "test_data['content'] = test_data['title'] + test_data['body']\n",
    "test_data = test_data.drop(columns=['title','body'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "test_data = test_data.sort_values(by='date')\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join(x.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_list = ['aviva','company','shenzhen','kate','euros','emirate','dhabi','metre','asia','europe','shanghai','nichola','roger','msci','iran','states','italy','united','china','usa','us','america','american','americans','chinese','china','russia','russian','putin','vladimir','trump','donald','biden','joe','ukraine','ukrainian','ukrainians','ukraines','ukraine','say','jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec','mon','tue','wed','thu','fri','sat','sun','monday','tuesday','wednesday','thursday','friday','saturday','sunday','january','february','march','april','may','june','july','august','september','october','november','december','today','yesterday','tomorrow','week','month','year','time','day','weekend','morning','afternoon','evening','night','news','new','news']\n",
    "list_numbers = ['eoi','name','houthi','uae','euro','yen','instead','liga','len','nhl','one','two','three','four','five','six','seven','eight','nine','ten','hundred','thousand','million','billion','trillion','first','second','third','eighted','series','hong','kong','new','york','los','angeles','san','francisco','las','vegas','san','diego','san','jose']\n",
    "country_list = ['myanmar','robert','lebanon','iivi','william','zalando','olympic','country','world','africa','China', 'United States of America', 'United Kingdom', 'France', 'Germany', 'Japan', 'Russia', 'Australia', 'Canada', 'India', 'Brazil', 'Italy', 'Spain', 'South Korea', 'Mexico', 'Netherlands', 'Switzerland', 'Sweden', 'Norway', 'Denmark', 'Finland', 'Greece', 'Ireland', 'Portugal', 'Poland', 'Ukraine', 'Romania', 'Belgium', 'Austria', 'Turkey', 'Saudi Arabia', 'United Arab Emirates', 'Iran', 'Iraq', 'Israel', 'Egypt', 'South Africa', 'Argentina', 'Venezuela', 'Thailand', 'Malaysia', 'Singapore', 'Indonesia', 'Philippines', 'Pakistan', 'Bangladesh', 'Nigeria', 'Kenya', 'Tanzania', 'Uganda']\n",
    "list_append = [\"whose\", \"german\", \"saidsign\", \"ceo\",\"exar\",\"chos\",\"sme\", \"vietnam\", \"gsk\", \"mori\", \"queen\", \"threeyear\",\"would\", \"come\", \"also\", \"could\", \"edit\", \"include\",\"pitch\", \"Britain\", \"Indian\", \"collin\", \"koo\", \"skorea\", \"men\", \"koo\", \"hub\",\"bbva\", \"korea\", \"inc\", \"btp\", \"ntpcs\", \"telecom\", \"omi\",\"jen\",\"andre\", \"spac\", \"sabadell\",\"faa\", \"unicredit\", \"city\", \"georgia\", \"puma\", \"philip\", \"england\", \"tokyo\", \"announce\", \"safrica\", \"andrea\"]\n",
    "stop_words.update(stop_words_list)\n",
    "stop_words.update(list_numbers)\n",
    "stop_words.update(country_list)\n",
    "stop_words.update(list_append)\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word != 'nan']))\n",
    "test_data['content_tokens'] = test_data['content'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>content_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36018</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>bomb cyclone batter alaskas aleutian island hu...</td>\n",
       "      <td>[bomb, cyclone, batter, alaskas, aleutian, isl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>pakistan inflation rate easkarachi pakistan pa...</td>\n",
       "      <td>[pakistan, inflation, rate, easkarachi, pakist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36084</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>australias batsman must grind foil india planm...</td>\n",
       "      <td>[australias, batsman, must, grind, foil, india...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36083</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>saint clear end isolation ahead liverpool clas...</td>\n",
       "      <td>[saint, clear, end, isolation, ahead, liverpoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36082</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>infectious covid variant find florida state of...</td>\n",
       "      <td>[infectious, covid, variant, find, florida, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36652</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>french town record covid case hinder drivemeau...</td>\n",
       "      <td>[french, town, record, covid, case, hinder, dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36618</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>booster slash omicron safrican studycape town ...</td>\n",
       "      <td>[booster, slash, omicron, safrican, studycape,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36619</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>dow close still poise big annual gainsummarydo...</td>\n",
       "      <td>[dow, close, still, poise, big, annual, gainsu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36614</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>insight woman force change indian iphone plant...</td>\n",
       "      <td>[insight, woman, force, change, indian, iphone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36606</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>atleticos simeone test covid positive barca ca...</td>\n",
       "      <td>[atleticos, simeone, test, covid, positive, ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47980 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                            content  \\\n",
       "36018 2021-01-01  bomb cyclone batter alaskas aleutian island hu...   \n",
       "36085 2021-01-01  pakistan inflation rate easkarachi pakistan pa...   \n",
       "36084 2021-01-01  australias batsman must grind foil india planm...   \n",
       "36083 2021-01-01  saint clear end isolation ahead liverpool clas...   \n",
       "36082 2021-01-01  infectious covid variant find florida state of...   \n",
       "...          ...                                                ...   \n",
       "36652 2021-12-30  french town record covid case hinder drivemeau...   \n",
       "36618 2021-12-30  booster slash omicron safrican studycape town ...   \n",
       "36619 2021-12-30  dow close still poise big annual gainsummarydo...   \n",
       "36614 2021-12-30  insight woman force change indian iphone plant...   \n",
       "36606 2021-12-30  atleticos simeone test covid positive barca ca...   \n",
       "\n",
       "                                          content_tokens  \n",
       "36018  [bomb, cyclone, batter, alaskas, aleutian, isl...  \n",
       "36085  [pakistan, inflation, rate, easkarachi, pakist...  \n",
       "36084  [australias, batsman, must, grind, foil, india...  \n",
       "36083  [saint, clear, end, isolation, ahead, liverpoo...  \n",
       "36082  [infectious, covid, variant, find, florida, st...  \n",
       "...                                                  ...  \n",
       "36652  [french, town, record, covid, case, hinder, dr...  \n",
       "36618  [booster, slash, omicron, safrican, studycape,...  \n",
       "36619  [dow, close, still, poise, big, annual, gainsu...  \n",
       "36614  [insight, woman, force, change, indian, iphone...  \n",
       "36606  [atleticos, simeone, test, covid, positive, ba...  \n",
       "\n",
       "[47980 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the date with less than 60 samples, output the data which can be used for the recollection\n",
    "start_date = '2021-01-01'\n",
    "end_date = '2021-12-31'\n",
    "test_data = test_data[(test_data['date'] >= start_date) & (test_data['date'] <= end_date)]\n",
    "test_data\n",
    "sample_counts = test_data['date'].value_counts()\n",
    "\n",
    "# find the dates with more than 60 samples and keep them as the test data\n",
    "valid_dates = sample_counts[sample_counts >= 30].index\n",
    "test_data = test_data[test_data['date'].isin(valid_dates)]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Silhouette parameters: Vector Size=70, Window=6, Min Count=3, N=20\n",
      "Best Calinski-Harabasz parameters: Vector Size=90, Window=8, Min Count=3, N=20\n"
     ]
    }
   ],
   "source": [
    "# Define parameter search space\n",
    "vector_sizes = [70, 80, 90]\n",
    "windows = [ 6,7, 8]\n",
    "min_counts = [2,3]  \n",
    "Ns = [10,15,20]\n",
    "\n",
    "best_silhouette_params = None\n",
    "best_silhouette_score = -1\n",
    "\n",
    "best_ch_params = None\n",
    "best_ch_score = -1\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window in windows:\n",
    "        for min_count in min_counts:\n",
    "            for N in Ns:\n",
    "                model = Word2Vec(sentences=test_data['content_tokens'], vector_size=vector_size, window=window, min_count=min_count, workers=8)\n",
    "                \n",
    "                corpus = test_data['content'].tolist()\n",
    "                vectorizer = TfidfVectorizer()\n",
    "                tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "                word2tfidf = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "            \n",
    "                test_data['important_words'] = test_data['content_tokens'].apply(\n",
    "                    lambda tokens: sorted(\n",
    "                        set(tokens),  # Remove duplicates\n",
    "                        key=lambda x: word2tfidf.get(x, 0),  # Rank by TF-IDF score\n",
    "                        reverse=False  # Lower TF-IDF means higher frequency in this case\n",
    "                    )[:N]\n",
    "                )\n",
    "                \n",
    "                def text_to_vector(text, model, word2tfidf):\n",
    "                    vectors = []\n",
    "                    weights = []\n",
    "                    for word in text:\n",
    "                        if word in model.wv:\n",
    "                            vectors.append(model.wv[word])\n",
    "                            weights.append(word2tfidf.get(word, 1.0))\n",
    "                    if not vectors:\n",
    "                        return np.zeros(model.vector_size)\n",
    "                    vectors = np.array(vectors)\n",
    "                    weights = np.array(weights) / sum(weights)\n",
    "                    return np.average(vectors, axis=0, weights=weights)\n",
    "                \n",
    "                test_data['vector'] = test_data['important_words'].apply(lambda x: text_to_vector(x, model, word2tfidf))\n",
    "                \n",
    "                X = np.array(test_data['vector'].tolist())\n",
    "                num_clusters = 2\n",
    "                kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "                labels = kmeans.fit_predict(X)\n",
    "                test_data['cluster_label'] = labels\n",
    "                \n",
    "                if len(set(labels)) > 1:\n",
    "                    silhouette = silhouette_score(X, labels)\n",
    "                    ch_score = calinski_harabasz_score(X, labels)\n",
    "                    \n",
    "                    if silhouette > best_silhouette_score: \n",
    "                        best_silhouette_score = silhouette\n",
    "                        best_silhouette_params = (vector_size, window, min_count, N)\n",
    "                    \n",
    "                    if ch_score > best_ch_score:\n",
    "                        best_ch_score = ch_score\n",
    "                        best_ch_params = (vector_size, window, min_count, N)\n",
    "                        \n",
    "print(f\"Best Silhouette parameters: Vector Size={best_silhouette_params[0]}, Window={best_silhouette_params[1]}, Min Count={best_silhouette_params[2]}, N={best_silhouette_params[3]}\")\n",
    "print(f\"Best Calinski-Harabasz parameters: Vector Size={best_ch_params[0]}, Window={best_ch_params[1]}, Min Count={best_ch_params[2]}, N={best_ch_params[3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
