{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import torch\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "test_data = pd.read_csv('data/processed_articles.csv')\n",
    "test_data['content'] = test_data['title'] + test_data['body']\n",
    "test_data = test_data.drop(columns=['title','body'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "test_data = test_data.sort_values(by='date')\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join(x.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_list = ['aviva','company','shenzhen','kate','euros','emirate','dhabi','metre','asia','europe','shanghai','nichola','roger','msci','iran','states','italy','united','china','usa','us','america','american','americans','chinese','china','russia','russian','putin','vladimir','trump','donald','biden','joe','ukraine','ukrainian','ukrainians','ukraines','ukraine','say','jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec','mon','tue','wed','thu','fri','sat','sun','monday','tuesday','wednesday','thursday','friday','saturday','sunday','january','february','march','april','may','june','july','august','september','october','november','december','today','yesterday','tomorrow','week','month','year','time','day','weekend','morning','afternoon','evening','night','news','new','news']\n",
    "list_numbers = ['eoi','name','houthi','uae','euro','yen','instead','liga','len','nhl','one','two','three','four','five','six','seven','eight','nine','ten','hundred','thousand','million','billion','trillion','first','second','third','eighted','series','hong','kong','new','york','los','angeles','san','francisco','las','vegas','san','diego','san','jose']\n",
    "country_list = ['myanmar','robert','lebanon','iivi','william','zalando','olympic','country','world','africa','China', 'United States of America', 'United Kingdom', 'France', 'Germany', 'Japan', 'Russia', 'Australia', 'Canada', 'India', 'Brazil', 'Italy', 'Spain', 'South Korea', 'Mexico', 'Netherlands', 'Switzerland', 'Sweden', 'Norway', 'Denmark', 'Finland', 'Greece', 'Ireland', 'Portugal', 'Poland', 'Ukraine', 'Romania', 'Belgium', 'Austria', 'Turkey', 'Saudi Arabia', 'United Arab Emirates', 'Iran', 'Iraq', 'Israel', 'Egypt', 'South Africa', 'Argentina', 'Venezuela', 'Thailand', 'Malaysia', 'Singapore', 'Indonesia', 'Philippines', 'Pakistan', 'Bangladesh', 'Nigeria', 'Kenya', 'Tanzania', 'Uganda']\n",
    "list_append = [\"whose\", \"german\", \"saidsign\", \"ceo\",\"exar\",\"chos\",\"sme\", \"vietnam\", \"gsk\", \"mori\", \"queen\", \"threeyear\",\"would\", \"come\", \"also\", \"could\", \"edit\", \"include\",\"pitch\", \"Britain\", \"Indian\", \"collin\", \"koo\", \"skorea\", \"men\", \"koo\", \"hub\",\"bbva\", \"korea\", \"inc\", \"btp\", \"ntpcs\", \"telecom\", \"omi\",\"jen\",\"andre\", \"spac\", \"sabadell\",\"faa\", \"unicredit\", \"city\", \"georgia\", \"puma\", \"philip\", \"england\", \"tokyo\", \"announce\", \"safrica\", \"andrea\"]\n",
    "stop_words.update(stop_words_list)\n",
    "stop_words.update(list_numbers)\n",
    "stop_words.update(country_list)\n",
    "stop_words.update(list_append)\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop_words)]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))\n",
    "test_data['content'] = test_data['content'].apply(lambda x: ' '.join([word for word in x.split() if word != 'nan']))\n",
    "test_data['content_tokens'] = test_data['content'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the date with less than 60 samples, output the data which can be used for the recollection\n",
    "start_date = '2021-01-01'\n",
    "end_date = '2021-12-31'\n",
    "test_data = test_data[(test_data['date'] >= start_date) & (test_data['date'] <= end_date)]\n",
    "test_data\n",
    "sample_counts = test_data['date'].value_counts()\n",
    "\n",
    "# find the dates with more than 60 samples and keep them as the test data\n",
    "valid_dates = sample_counts[sample_counts >= 30].index\n",
    "test_data = test_data[test_data['date'].isin(valid_dates)]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter search space\n",
    "vector_sizes = [70, 80, 90]\n",
    "windows = [ 6,7, 8]\n",
    "min_counts = [2,3]  \n",
    "Ns = [10,15,20]\n",
    "\n",
    "best_silhouette_params = None\n",
    "best_silhouette_score = -1\n",
    "\n",
    "best_ch_params = None\n",
    "best_ch_score = -1\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window in windows:\n",
    "        for min_count in min_counts:\n",
    "            for N in Ns:\n",
    "                model = Word2Vec(sentences=test_data['content_tokens'], vector_size=vector_size, window=window, min_count=min_count, workers=8)\n",
    "                \n",
    "                corpus = test_data['content'].tolist()\n",
    "                vectorizer = TfidfVectorizer()\n",
    "                tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "                word2tfidf = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "            \n",
    "                test_data['important_words'] = test_data['content_tokens'].apply(\n",
    "                    lambda tokens: sorted(\n",
    "                        set(tokens),  # Remove duplicates\n",
    "                        key=lambda x: word2tfidf.get(x, 0),  # Rank by TF-IDF score\n",
    "                        reverse=False  # Lower TF-IDF means higher frequency in this case\n",
    "                    )[:N]\n",
    "                )\n",
    "                \n",
    "                def text_to_vector(text, model, word2tfidf):\n",
    "                    vectors = []\n",
    "                    weights = []\n",
    "                    for word in text:\n",
    "                        if word in model.wv:\n",
    "                            vectors.append(model.wv[word])\n",
    "                            weights.append(word2tfidf.get(word, 1.0))\n",
    "                    if not vectors:\n",
    "                        return np.zeros(model.vector_size)\n",
    "                    vectors = np.array(vectors)\n",
    "                    weights = np.array(weights) / sum(weights)\n",
    "                    return np.average(vectors, axis=0, weights=weights)\n",
    "                \n",
    "                test_data['vector'] = test_data['important_words'].apply(lambda x: text_to_vector(x, model, word2tfidf))\n",
    "                \n",
    "                X = np.array(test_data['vector'].tolist())\n",
    "                num_clusters = 2\n",
    "                kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "                labels = kmeans.fit_predict(X)\n",
    "                test_data['cluster_label'] = labels\n",
    "                \n",
    "                if len(set(labels)) > 1:\n",
    "                    silhouette = silhouette_score(X, labels)\n",
    "                    ch_score = calinski_harabasz_score(X, labels)\n",
    "                    \n",
    "                    if silhouette > best_silhouette_score: \n",
    "                        best_silhouette_score = silhouette\n",
    "                        best_silhouette_params = (vector_size, window, min_count, N)\n",
    "                    \n",
    "                    if ch_score > best_ch_score:\n",
    "                        best_ch_score = ch_score\n",
    "                        best_ch_params = (vector_size, window, min_count, N)\n",
    "                        \n",
    "print(f\"Best Silhouette parameters: Vector Size={best_silhouette_params[0]}, Window={best_silhouette_params[1]}, Min Count={best_silhouette_params[2]}, N={best_silhouette_params[3]}\")\n",
    "print(f\"Best Calinski-Harabasz parameters: Vector Size={best_ch_params[0]}, Window={best_ch_params[1]}, Min Count={best_ch_params[2]}, N={best_ch_params[3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
