{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b69f184-49fa-436b-88c1-86824c97300f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reuters.com/archive/2014-01/01/1/\n",
      "https://www.reuters.com/archive/2014-01/02/1/\n",
      "https://www.reuters.com/archive/2014-01/03/1/\n",
      "https://www.reuters.com/archive/2014-01/04/1/\n",
      "https://www.reuters.com/archive/2014-01/05/1/\n",
      "https://www.reuters.com/archive/2014-01/06/1/\n",
      "https://www.reuters.com/archive/2014-01/07/1/\n",
      "https://www.reuters.com/archive/2014-01/08/1/\n",
      "https://www.reuters.com/archive/2014-01/09/1/\n",
      "https://www.reuters.com/archive/2014-01/10/1/\n",
      "https://www.reuters.com/archive/2014-01/11/1/\n",
      "https://www.reuters.com/archive/2014-01/12/1/\n",
      "https://www.reuters.com/archive/2014-01/13/1/\n",
      "https://www.reuters.com/archive/2014-01/14/1/\n",
      "https://www.reuters.com/archive/2014-01/15/1/\n",
      "https://www.reuters.com/archive/2014-01/16/1/\n",
      "https://www.reuters.com/archive/2014-01/17/1/\n",
      "https://www.reuters.com/archive/2014-01/18/1/\n",
      "https://www.reuters.com/archive/2014-01/19/1/\n",
      "https://www.reuters.com/archive/2014-01/20/1/\n",
      "https://www.reuters.com/archive/2014-01/21/1/\n",
      "https://www.reuters.com/archive/2014-01/22/1/\n",
      "https://www.reuters.com/archive/2014-01/23/1/\n",
      "https://www.reuters.com/archive/2014-01/24/1/\n",
      "https://www.reuters.com/archive/2014-01/25/1/\n",
      "https://www.reuters.com/archive/2014-01/26/1/\n",
      "https://www.reuters.com/archive/2014-01/27/1/\n",
      "https://www.reuters.com/archive/2014-01/28/1/\n",
      "https://www.reuters.com/archive/2014-01/29/1/\n",
      "https://www.reuters.com/archive/2014-01/30/1/\n",
      "https://www.reuters.com/archive/2014-01/31/1/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_page(url):\n",
    "    '''\n",
    "        Fetch page response, which is a full html\n",
    "    '''\n",
    "    curl_command = [\n",
    "        'curl', url,\n",
    "        '-H', 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        '-H', 'accept-language: zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        '-H', 'sec-fetch-dest: document',\n",
    "        '-H', 'sec-fetch-mode: navigate',\n",
    "        '-H', 'sec-fetch-site: none',\n",
    "        '-H', 'sec-fetch-user: ?1',\n",
    "        '-H', 'upgrade-insecure-requests: 1',\n",
    "        '-H', 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(curl_command, capture_output=True, text=True)\n",
    "    return result.stdout if result.returncode == 0 else None\n",
    "\n",
    "def fetch_article(url):\n",
    "    '''\n",
    "        Fetch an article as a full html.\n",
    "        Handle redirection issue.\n",
    "    '''\n",
    "    curl_command = [\n",
    "        'curl', '-i', url,\n",
    "        '-H', 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        '-H', 'accept-language: zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        '-b', '_awl=2.1740198990.5-404e3f6e73e257dac52922cf63761642-6763652d617369612d6561737431-7; _chartbeat2=.1705476051042.1740198990957.0000000100100001.CPMAl_EV33CBYEv5GlgcVZDhfqVU.1; _cb_svref=external; dicbo_id=%7B%22dicbo_fetch%22%3A1740198991413%7D; datadome=KsKBUmnX5wD9DYXmSxENthQPyM4B3zge5fMF5mI1e__ZOO15Sim_GaHji473RrW3ArTLjguYiu~UFv1GTFmQ9Qbnf2N9vml8uCd3wrAiRmrk0Nly~LBKRZvcYUGmfawK',\n",
    "        '-H', 'priority: u=0, i',\n",
    "        '-H', 'sec-ch-device-memory: 8',\n",
    "        '-H', 'sec-ch-ua: \"Not(A:Brand\";v=\"99\", \"Google Chrome\";v=\"133\", \"Chromium\";v=\"133\"',\n",
    "        '-H', 'sec-ch-ua-arch: \"arm\"',\n",
    "        '-H', 'sec-ch-ua-full-version-list: \"Not(A:Brand\";v=\"99.0.0.0\", \"Google Chrome\";v=\"133.0.6943.127\", \"Chromium\";v=\"133.0.6943.127\"',\n",
    "        '-H', 'sec-ch-ua-mobile: ?0',\n",
    "        '-H', 'sec-ch-ua-model: \"\"',\n",
    "        '-H', 'sec-ch-ua-platform: \"macOS\"',\n",
    "        '-H', 'sec-fetch-dest: document',\n",
    "        '-H', 'sec-fetch-mode: navigate',\n",
    "        '-H', 'sec-fetch-site: none',\n",
    "        '-H', 'sec-fetch-user: ?1',\n",
    "        '-H', 'upgrade-insecure-requests: 1',\n",
    "        '-H', 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(curl_command, capture_output=True, text=True)\n",
    "    headers, _, body = result.stdout.partition(\"\\n\\n\")\n",
    "    \n",
    "    for line in headers.split(\"\\n\"):\n",
    "        if line.lower().startswith(\"location: \"):\n",
    "            redirected_url = line.split(\": \", 1)[1].strip()\n",
    "            #print(redirected_url)\n",
    "            return fetch_article(redirected_url)\n",
    "    \n",
    "    return body if result.returncode == 0 else None\n",
    "\n",
    "def parse_archive_page(html):\n",
    "    '''\n",
    "        Parse archive page to get article list.\n",
    "    '''\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = []\n",
    "    \n",
    "    for link in soup.find_all('a', {'data-testid': 'TitleLink'}):\n",
    "        href = link.get('href')\n",
    "        if href.startswith(\"/article/\"):\n",
    "            articles.append(\"https://www.reuters.com\" + href)\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def parse_article_page(html):\n",
    "    '''\n",
    "        Parse article html to construct a json file.\n",
    "    '''\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    title = soup.find('title').get_text(strip=True) if soup.find('title') else \"\"\n",
    "    \n",
    "    date_info = soup.find_all(class_='date-line__date___kNbY')\n",
    "    date, time_, updated = [d.get_text(strip=True) for d in date_info[:3]] if len(date_info) >= 3 else (\"\", \"\", \"\")\n",
    "    \n",
    "    body = \"\".join([p.get_text(strip=True) for p in soup.find_all(class_='article-body__content__17Yit')])\n",
    "    \n",
    "    tags = [tag.get_text(strip=True) for tag in soup.find_all(attrs={'aria-label': 'Tags'})]\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"date\": date,\n",
    "        \"time\": time_,\n",
    "        \"updated\": updated,\n",
    "        \"body\": body,\n",
    "        \"tags\": tags\n",
    "    }\n",
    "\n",
    "def save_article(article, date_str):\n",
    "    '''\n",
    "        Save article content as a json file, the path is YYYYMMDD/{Title}.json\n",
    "    '''\n",
    "    folder = f\"articles/{date_str}\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    file_name = os.path.join(folder, f\"{article['title'].replace('/', '_')}.json\")\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        json.dump(article, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "        Main scrap process.\n",
    "    '''\n",
    "    # Page url\n",
    "    base_url = \"https://www.reuters.com/archive/{}/{}/{}/\"\n",
    "\n",
    "    # Start and end date\n",
    "    start_date = datetime(2014, 1, 1)\n",
    "    end_date = datetime(2014, 1, 31)\n",
    "    # Iteration date\n",
    "    current_date = start_date\n",
    "\n",
    "    # Iterate all the dates\n",
    "    while current_date <= end_date:\n",
    "        yyyy_mm = current_date.strftime(\"%Y-%m\")\n",
    "        dd = current_date.strftime(\"%d\")\n",
    "        page = 1\n",
    "        \n",
    "        while True:\n",
    "            archive_url = base_url.format(yyyy_mm, dd, page)\n",
    "            print(archive_url)\n",
    "            html = fetch_page(archive_url)\n",
    "            if not html:\n",
    "                break\n",
    "            \n",
    "            article_links = parse_archive_page(html)\n",
    "\n",
    "            # Keep request the archive page by pageNo, until there is no more article.\n",
    "            if not article_links:\n",
    "                break\n",
    "\n",
    "            # Iterate the article list\n",
    "            for article_url in article_links:\n",
    "                #print(article_url)\n",
    "                # Fetch article content\n",
    "                article_html = fetch_article(article_url)\n",
    "                if article_html:\n",
    "                    article_data = parse_article_page(article_html)\n",
    "                    print(article_data['title'])\n",
    "                    save_article(article_data, current_date.strftime(\"%Y%m%d\"))\n",
    "                # Sleep for 3 seconds\n",
    "                time.sleep(3)\n",
    "            \n",
    "            page += 1\n",
    "            time.sleep(5)\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfe0a8-7425-49b5-81c3-83a2c1a32dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f81a6-0c6e-41d1-9147-aa1cda922ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
