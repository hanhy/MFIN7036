{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616bf8c-106d-4bac-9cd3-30a4e56d9596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_date=2021-08-04\n",
      "Sleep for 120 seconds before retry\n",
      "Sleep for 180 seconds before retry\n",
      "Sleep for 240 seconds before retry\n",
      "Sleep for 300 seconds before retry\n",
      "not links\n",
      "Totally 2 pages for 2021-08-04, total num is 5\n",
      "Saved 5 links to articles_links\\20210804.txt\n",
      "current_date=2021-08-05\n",
      "Sleep for 120 seconds before retry\n",
      "Sleep for 180 seconds before retry\n",
      "Sleep for 240 seconds before retry\n",
      "Sleep for 300 seconds before retry\n",
      "Sleep for 360 seconds before retry\n",
      "Sleep for 420 seconds before retry\n",
      "10 pages for 2021-08-05, total num is 33\n",
      "20 pages for 2021-08-05, total num is 84\n",
      "not links\n",
      "Totally 21 pages for 2021-08-05, total num is 90\n",
      "Saved 90 links to articles_links\\20210805.txt\n",
      "current_date=2021-08-06\n",
      "Sleep for 120 seconds before retry\n",
      "Sleep for 180 seconds before retry\n",
      "Sleep for 240 seconds before retry\n",
      "Sleep for 300 seconds before retry\n",
      "Sleep for 360 seconds before retry\n",
      "Sleep for 420 seconds before retry\n",
      "not links\n",
      "Totally 2 pages for 2021-08-06, total num is 6\n",
      "Saved 6 links to articles_links\\20210806.txt\n",
      "current_date=2021-08-07\n",
      "Sleep for 120 seconds before retry\n",
      "Sleep for 180 seconds before retry\n",
      "Sleep for 240 seconds before retry\n",
      "Sleep for 300 seconds before retry\n",
      "Sleep for 360 seconds before retry\n",
      "Sleep for 420 seconds before retry\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "def fetch_page(url):\n",
    "    curl_command = [\n",
    "        'curl', url,\n",
    "        '-H', 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        '-H', 'accept-language: zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        '-H', 'sec-fetch-dest: document',\n",
    "        '-H', 'sec-fetch-mode: navigate',\n",
    "        '-H', 'sec-fetch-site: none',\n",
    "        '-H', 'sec-fetch-user: ?1',\n",
    "        '-H', 'upgrade-insecure-requests: 1',\n",
    "        '-H', 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(curl_command, capture_output=True, text=True)\n",
    "    #print(result)\n",
    "    return result.stdout if result.returncode == 0 else None\n",
    "\n",
    "def fetch_article(url):\n",
    "    curl_command = [\n",
    "        'curl', '-i', url,\n",
    "        '-H', 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        '-H', 'accept-language: zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        '-b', '_awl=2.1740198990.5-404e3f6e73e257dac52922cf63761642-6763652d617369612d6561737431-7; _chartbeat2=.1705476051042.1740198990957.0000000100100001.CPMAl_EV33CBYEv5GlgcVZDhfqVU.1; _cb_svref=external; dicbo_id=%7B%22dicbo_fetch%22%3A1740198991413%7D; datadome=KsKBUmnX5wD9DYXmSxENthQPyM4B3zge5fMF5mI1e__ZOO15Sim_GaHji473RrW3ArTLjguYiu~UFv1GTFmQ9Qbnf2N9vml8uCd3wrAiRmrk0Nly~LBKRZvcYUGmfawK',\n",
    "        '-H', 'priority: u=0, i',\n",
    "        '-H', 'sec-ch-device-memory: 8',\n",
    "        '-H', 'sec-ch-ua: \"Not(A:Brand\";v=\"99\", \"Google Chrome\";v=\"133\", \"Chromium\";v=\"133\"',\n",
    "        '-H', 'sec-ch-ua-arch: \"arm\"',\n",
    "        '-H', 'sec-ch-ua-full-version-list: \"Not(A:Brand\";v=\"99.0.0.0\", \"Google Chrome\";v=\"133.0.6943.127\", \"Chromium\";v=\"133.0.6943.127\"',\n",
    "        '-H', 'sec-ch-ua-mobile: ?0',\n",
    "        '-H', 'sec-ch-ua-model: \"\"',\n",
    "        '-H', 'sec-ch-ua-platform: \"macOS\"',\n",
    "        '-H', 'sec-fetch-dest: document',\n",
    "        '-H', 'sec-fetch-mode: navigate',\n",
    "        '-H', 'sec-fetch-site: none',\n",
    "        '-H', 'sec-fetch-user: ?1',\n",
    "        '-H', 'upgrade-insecure-requests: 1',\n",
    "        '-H', 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(curl_command, capture_output=True, text=True)\n",
    "    headers, _, body = result.stdout.partition(\"\\n\\n\")\n",
    "    \n",
    "    for line in headers.split(\"\\n\"):\n",
    "        if line.lower().startswith(\"location: \"):\n",
    "            redirected_url = line.split(\": \", 1)[1].strip()\n",
    "            print(redirected_url)\n",
    "            return fetch_article(redirected_url)\n",
    "    \n",
    "    return body if result.returncode == 0 else None\n",
    "\n",
    "def parse_archive_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = []\n",
    "    \n",
    "    for link in soup.find_all('a', {'data-testid': 'TitleLink'}):\n",
    "        href = link.get('href')\n",
    "        if href.startswith(\"/article/\"):\n",
    "            articles.append(\"https://www.reuters.com\" + href)\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def parse_article_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    title = soup.find('title').get_text(strip=True) if soup.find('title') else \"\"\n",
    "    \n",
    "    date_info = soup.find_all(class_='date-line__date___kNbY')\n",
    "    date, time_, updated = [d.get_text(strip=True) for d in date_info[:3]] if len(date_info) >= 3 else (\"\", \"\", \"\")\n",
    "    \n",
    "    body = \"\".join([p.get_text(strip=True) for p in soup.find_all(class_='article-body__content__17Yit')])\n",
    "    \n",
    "    tags = [tag.get_text(strip=True) for tag in soup.find_all(attrs={'aria-label': 'Tags'})]\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"date\": date,\n",
    "        \"time\": time_,\n",
    "        \"updated\": updated,\n",
    "        \"body\": body,\n",
    "        \"tags\": tags\n",
    "    }\n",
    "\n",
    "def save_articles_links(current_date, article_links):\n",
    "    # 格式化日期为 YYYYMMDD\n",
    "    date_str = current_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # 设置保存目录和文件路径\n",
    "    output_dir = \"articles_links\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    output_file = os.path.join(output_dir, f\"{date_str}.txt\")\n",
    "    \n",
    "    # 将列表内容写入文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for link in article_links:\n",
    "            f.write(f\"{link}\\n\")\n",
    "    \n",
    "    print(f\"Saved {len(article_links)} links to {output_file}\")\n",
    "    \n",
    "def main():\n",
    "    base_url = \"https://www.reuters.com/archive/{}/{}/{}/\"\n",
    "    start_date = datetime(2021, 8, 4)\n",
    "    end_date = datetime(2021, 10, 31)\n",
    "    current_date = start_date\n",
    "    #time.sleep(3600)\n",
    "    while current_date <= end_date:\n",
    "        time.sleep(random.uniform(60, 100))\n",
    "        yyyy_mm = current_date.strftime(\"%Y-%m\")\n",
    "        dd = current_date.strftime(\"%d\")\n",
    "        print(f'current_date={current_date.strftime(\"%Y-%m-%d\")}')\n",
    "        page = 1\n",
    "\n",
    "        article_links = set([])\n",
    "        retry_sleep_time = 60 # If need retry, time should sleep for a while before retry\n",
    "    \n",
    "        while True:\n",
    "            archive_url = base_url.format(yyyy_mm, dd, page)\n",
    "            html = fetch_page(archive_url)\n",
    "            #print(html)\n",
    "            if not html:\n",
    "                break\n",
    "            \n",
    "            links = parse_archive_page(html)\n",
    "            article_links = article_links | set(links)\n",
    "            \n",
    "            if len(links) < 2:\n",
    "                if page == 1:\n",
    "                    # no enough links for first page\n",
    "                    article_links.clear()\n",
    "                    \n",
    "                    retry_flag = True\n",
    "                    retry_sleep_time += 60 # every time you encounter a block issue, sleep 60 seconds more\n",
    "                    print(f'Sleep for {retry_sleep_time} seconds before retry')\n",
    "                    time.sleep(retry_sleep_time)\n",
    "                    continue\n",
    "                    \n",
    "                #print(html)\n",
    "                print('not links')\n",
    "                break\n",
    "            else:\n",
    "                retry_sleep_time = 60\n",
    "\n",
    "            page += 1\n",
    "            if page % 10 == 0:\n",
    "                print(f'{page} pages for {current_date.strftime(\"%Y-%m-%d\")}, total num is {len(article_links)}')\n",
    "            time.sleep(random.uniform(1, 15))\n",
    "        print(f'Totally {page} pages for {current_date.strftime(\"%Y-%m-%d\")}, total num is {len(article_links)}')\n",
    "        save_articles_links(current_date, article_links)\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
