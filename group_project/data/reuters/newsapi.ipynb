{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a616bf8c-106d-4bac-9cd3-30a4e56d9596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_date=2019-02-05\n",
      "10 pages for 2019-02-05, total num is 62\n",
      "20 pages for 2019-02-05, total num is 124\n",
      "30 pages for 2019-02-05, total num is 164\n",
      "40 pages for 2019-02-05, total num is 225\n",
      "50 pages for 2019-02-05, total num is 285\n",
      "60 pages for 2019-02-05, total num is 336\n",
      "70 pages for 2019-02-05, total num is 391\n",
      "80 pages for 2019-02-05, total num is 440\n",
      "90 pages for 2019-02-05, total num is 482\n",
      "100 pages for 2019-02-05, total num is 534\n",
      "110 pages for 2019-02-05, total num is 588\n",
      "120 pages for 2019-02-05, total num is 633\n",
      "130 pages for 2019-02-05, total num is 685\n",
      "not links\n",
      "Totally 131 pages for 2019-02-05, total num is 686\n",
      "Saved 686 links to articles_links/20190205.txt\n",
      "current_date=2019-02-06\n",
      "10 pages for 2019-02-06, total num is 53\n",
      "20 pages for 2019-02-06, total num is 99\n",
      "30 pages for 2019-02-06, total num is 150\n",
      "40 pages for 2019-02-06, total num is 209\n",
      "50 pages for 2019-02-06, total num is 274\n",
      "60 pages for 2019-02-06, total num is 325\n",
      "70 pages for 2019-02-06, total num is 371\n",
      "80 pages for 2019-02-06, total num is 424\n",
      "90 pages for 2019-02-06, total num is 469\n",
      "100 pages for 2019-02-06, total num is 511\n",
      "110 pages for 2019-02-06, total num is 574\n",
      "120 pages for 2019-02-06, total num is 623\n",
      "130 pages for 2019-02-06, total num is 678\n",
      "not links\n",
      "Totally 135 pages for 2019-02-06, total num is 704\n",
      "Saved 704 links to articles_links/20190206.txt\n",
      "current_date=2019-02-07\n",
      "10 pages for 2019-02-07, total num is 50\n",
      "20 pages for 2019-02-07, total num is 109\n",
      "30 pages for 2019-02-07, total num is 160\n",
      "40 pages for 2019-02-07, total num is 226\n",
      "50 pages for 2019-02-07, total num is 284\n",
      "60 pages for 2019-02-07, total num is 327\n",
      "70 pages for 2019-02-07, total num is 379\n",
      "80 pages for 2019-02-07, total num is 434\n",
      "90 pages for 2019-02-07, total num is 489\n",
      "100 pages for 2019-02-07, total num is 534\n",
      "not links\n",
      "Totally 104 pages for 2019-02-07, total num is 554\n",
      "Saved 554 links to articles_links/20190207.txt\n",
      "current_date=2019-02-08\n",
      "10 pages for 2019-02-08, total num is 51\n",
      "20 pages for 2019-02-08, total num is 105\n",
      "30 pages for 2019-02-08, total num is 160\n",
      "40 pages for 2019-02-08, total num is 207\n",
      "50 pages for 2019-02-08, total num is 273\n",
      "60 pages for 2019-02-08, total num is 323\n",
      "70 pages for 2019-02-08, total num is 372\n",
      "80 pages for 2019-02-08, total num is 424\n",
      "90 pages for 2019-02-08, total num is 473\n",
      "100 pages for 2019-02-08, total num is 522\n",
      "110 pages for 2019-02-08, total num is 566\n",
      "120 pages for 2019-02-08, total num is 624\n",
      "130 pages for 2019-02-08, total num is 668\n",
      "not links\n",
      "Totally 134 pages for 2019-02-08, total num is 683\n",
      "Saved 683 links to articles_links/20190208.txt\n",
      "current_date=2019-02-09\n",
      "not links\n",
      "Totally 7 pages for 2019-02-09, total num is 27\n",
      "Saved 27 links to articles_links/20190209.txt\n",
      "current_date=2019-02-10\n",
      "Sleep for 120 seconds before retry\n",
      "Sleep for 180 seconds before retry\n",
      "Sleep for 240 seconds before retry\n",
      "Sleep for 300 seconds before retry\n",
      "Sleep for 360 seconds before retry\n",
      "Sleep for 420 seconds before retry\n",
      "Sleep for 480 seconds before retry\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 162\u001b[0m\n\u001b[1;32m    157\u001b[0m         save_articles_links(current_date, article_links)\n\u001b[1;32m    159\u001b[0m         current_date \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[1], line 143\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m     retry_sleep_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;66;03m# every time you encounter a block issue, sleep 60 seconds more\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSleep for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_sleep_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds before retry\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 143\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(retry_sleep_time)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m#print(html)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "def fetch_page(url):\n",
    "    curl_command = [\n",
    "        'curl', url,\n",
    "        '-H', 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        '-H', 'accept-language: zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        '-H', 'sec-fetch-dest: document',\n",
    "        '-H', 'sec-fetch-mode: navigate',\n",
    "        '-H', 'sec-fetch-site: none',\n",
    "        '-H', 'sec-fetch-user: ?1',\n",
    "        '-H', 'upgrade-insecure-requests: 1',\n",
    "        '-H', 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(curl_command, capture_output=True, text=True)\n",
    "    #print(result)\n",
    "    return result.stdout if result.returncode == 0 else None\n",
    "\n",
    "def fetch_article(url):\n",
    "    curl_command = [\n",
    "        'curl', '-i', url,\n",
    "        '-H', 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        '-H', 'accept-language: zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        '-b', '_awl=2.1740198990.5-404e3f6e73e257dac52922cf63761642-6763652d617369612d6561737431-7; _chartbeat2=.1705476051042.1740198990957.0000000100100001.CPMAl_EV33CBYEv5GlgcVZDhfqVU.1; _cb_svref=external; dicbo_id=%7B%22dicbo_fetch%22%3A1740198991413%7D; datadome=KsKBUmnX5wD9DYXmSxENthQPyM4B3zge5fMF5mI1e__ZOO15Sim_GaHji473RrW3ArTLjguYiu~UFv1GTFmQ9Qbnf2N9vml8uCd3wrAiRmrk0Nly~LBKRZvcYUGmfawK',\n",
    "        '-H', 'priority: u=0, i',\n",
    "        '-H', 'sec-ch-device-memory: 8',\n",
    "        '-H', 'sec-ch-ua: \"Not(A:Brand\";v=\"99\", \"Google Chrome\";v=\"133\", \"Chromium\";v=\"133\"',\n",
    "        '-H', 'sec-ch-ua-arch: \"arm\"',\n",
    "        '-H', 'sec-ch-ua-full-version-list: \"Not(A:Brand\";v=\"99.0.0.0\", \"Google Chrome\";v=\"133.0.6943.127\", \"Chromium\";v=\"133.0.6943.127\"',\n",
    "        '-H', 'sec-ch-ua-mobile: ?0',\n",
    "        '-H', 'sec-ch-ua-model: \"\"',\n",
    "        '-H', 'sec-ch-ua-platform: \"macOS\"',\n",
    "        '-H', 'sec-fetch-dest: document',\n",
    "        '-H', 'sec-fetch-mode: navigate',\n",
    "        '-H', 'sec-fetch-site: none',\n",
    "        '-H', 'sec-fetch-user: ?1',\n",
    "        '-H', 'upgrade-insecure-requests: 1',\n",
    "        '-H', 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(curl_command, capture_output=True, text=True)\n",
    "    headers, _, body = result.stdout.partition(\"\\n\\n\")\n",
    "    \n",
    "    for line in headers.split(\"\\n\"):\n",
    "        if line.lower().startswith(\"location: \"):\n",
    "            redirected_url = line.split(\": \", 1)[1].strip()\n",
    "            print(redirected_url)\n",
    "            return fetch_article(redirected_url)\n",
    "    \n",
    "    return body if result.returncode == 0 else None\n",
    "\n",
    "def parse_archive_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = []\n",
    "    \n",
    "    for link in soup.find_all('a', {'data-testid': 'TitleLink'}):\n",
    "        href = link.get('href')\n",
    "        if href.startswith(\"/article/\"):\n",
    "            articles.append(\"https://www.reuters.com\" + href)\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def parse_article_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    title = soup.find('title').get_text(strip=True) if soup.find('title') else \"\"\n",
    "    \n",
    "    date_info = soup.find_all(class_='date-line__date___kNbY')\n",
    "    date, time_, updated = [d.get_text(strip=True) for d in date_info[:3]] if len(date_info) >= 3 else (\"\", \"\", \"\")\n",
    "    \n",
    "    body = \"\".join([p.get_text(strip=True) for p in soup.find_all(class_='article-body__content__17Yit')])\n",
    "    \n",
    "    tags = [tag.get_text(strip=True) for tag in soup.find_all(attrs={'aria-label': 'Tags'})]\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"date\": date,\n",
    "        \"time\": time_,\n",
    "        \"updated\": updated,\n",
    "        \"body\": body,\n",
    "        \"tags\": tags\n",
    "    }\n",
    "\n",
    "def save_articles_links(current_date, article_links):\n",
    "    # 格式化日期为 YYYYMMDD\n",
    "    date_str = current_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # 设置保存目录和文件路径\n",
    "    output_dir = \"articles_links\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    output_file = os.path.join(output_dir, f\"{date_str}.txt\")\n",
    "    \n",
    "    # 将列表内容写入文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for link in article_links:\n",
    "            f.write(f\"{link}\\n\")\n",
    "    \n",
    "    print(f\"Saved {len(article_links)} links to {output_file}\")\n",
    "    \n",
    "def main():\n",
    "    base_url = \"https://www.reuters.com/archive/{}/{}/{}/\"\n",
    "    start_date = datetime(2019, 2, 9)\n",
    "    end_date = datetime(2019, 12, 31)\n",
    "    current_date = start_date\n",
    "    #time.sleep(3600)\n",
    "    while current_date <= end_date:\n",
    "        time.sleep(random.uniform(60, 100))\n",
    "        yyyy_mm = current_date.strftime(\"%Y-%m\")\n",
    "        dd = current_date.strftime(\"%d\")\n",
    "        print(f'current_date={current_date.strftime(\"%Y-%m-%d\")}')\n",
    "        page = 1\n",
    "\n",
    "        article_links = set([])\n",
    "        retry_sleep_time = 60 # If need retry, time should sleep for a while before retry\n",
    "    \n",
    "        while True:\n",
    "            archive_url = base_url.format(yyyy_mm, dd, page)\n",
    "            html = fetch_page(archive_url)\n",
    "            #print(html)\n",
    "            if not html:\n",
    "                break\n",
    "            \n",
    "            links = parse_archive_page(html)\n",
    "            article_links = article_links | set(links)\n",
    "            \n",
    "            if len(links) < 2:\n",
    "                if page == 1:\n",
    "                    # no enough links for first page\n",
    "                    article_links.clear()\n",
    "                    \n",
    "                    retry_flag = True\n",
    "                    retry_sleep_time += 60 # every time you encounter a block issue, sleep 60 seconds more\n",
    "                    print(f'Sleep for {retry_sleep_time} seconds before retry')\n",
    "                    time.sleep(retry_sleep_time)\n",
    "                    continue\n",
    "                    \n",
    "                #print(html)\n",
    "                print('not links')\n",
    "                break\n",
    "            else:\n",
    "                retry_sleep_time = 60\n",
    "\n",
    "            page += 1\n",
    "            if page % 10 == 0:\n",
    "                print(f'{page} pages for {current_date.strftime(\"%Y-%m-%d\")}, total num is {len(article_links)}')\n",
    "            time.sleep(random.uniform(1, 15))\n",
    "        print(f'Totally {page} pages for {current_date.strftime(\"%Y-%m-%d\")}, total num is {len(article_links)}')\n",
    "        save_articles_links(current_date, article_links)\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9f9a7-4799-44f0-a753-f5df87c7e9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
