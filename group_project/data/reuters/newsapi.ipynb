{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616bf8c-106d-4bac-9cd3-30a4e56d9596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time:2025-03-02 12:41:03.289603\n",
      "Run days=0\n",
      "current_date=2021-04-06\n",
      "10 pages for 2021-04-06, total num is 42\n",
      "20 pages for 2021-04-06, total num is 90\n",
      "30 pages for 2021-04-06, total num is 134\n",
      "40 pages for 2021-04-06, total num is 190\n",
      "50 pages for 2021-04-06, total num is 241\n",
      "60 pages for 2021-04-06, total num is 292\n",
      "70 pages for 2021-04-06, total num is 339\n",
      "80 pages for 2021-04-06, total num is 383\n",
      "90 pages for 2021-04-06, total num is 439\n",
      "100 pages for 2021-04-06, total num is 483\n",
      "110 pages for 2021-04-06, total num is 535\n",
      "not links\n",
      "Totally 116 pages for 2021-04-06, total num is 559\n",
      "Saved 559 links to articles_links/20210406.txt\n",
      "Current time:2025-03-02 12:43:45.465261\n",
      "Run days=1\n",
      "current_date=2021-04-07\n",
      "10 pages for 2021-04-07, total num is 42\n",
      "20 pages for 2021-04-07, total num is 95\n",
      "30 pages for 2021-04-07, total num is 151\n",
      "40 pages for 2021-04-07, total num is 199\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import requests\n",
    "\n",
    "def fetch_page(url):\n",
    "    curl_command = [\n",
    "        'curl', url,\n",
    "        '-H', 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        '-H', 'accept-language: zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        '-H', 'sec-fetch-dest: document',\n",
    "        '-H', 'sec-fetch-mode: navigate',\n",
    "        '-H', 'sec-fetch-site: none',\n",
    "        '-H', 'sec-fetch-user: ?1',\n",
    "        '-H', 'upgrade-insecure-requests: 1',\n",
    "        #'-H', 'user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        '-H', 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(curl_command, capture_output=True, text=True)\n",
    "    #print(result)\n",
    "    return result.stdout if result.returncode == 0 else None\n",
    "\n",
    "def fetch_article(url):\n",
    "    curl_command = [\n",
    "        'curl', '-i', url,\n",
    "        '-H', 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        '-H', 'accept-language: zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        '-b', '_awl=2.1740198990.5-404e3f6e73e257dac52922cf63761642-6763652d617369612d6561737431-7; _chartbeat2=.1705476051042.1740198990957.0000000100100001.CPMAl_EV33CBYEv5GlgcVZDhfqVU.1; _cb_svref=external; dicbo_id=%7B%22dicbo_fetch%22%3A1740198991413%7D; datadome=KsKBUmnX5wD9DYXmSxENthQPyM4B3zge5fMF5mI1e__ZOO15Sim_GaHji473RrW3ArTLjguYiu~UFv1GTFmQ9Qbnf2N9vml8uCd3wrAiRmrk0Nly~LBKRZvcYUGmfawK',\n",
    "        '-H', 'priority: u=0, i',\n",
    "        '-H', 'sec-ch-device-memory: 8',\n",
    "        '-H', 'sec-ch-ua: \"Not(A:Brand\";v=\"99\", \"Google Chrome\";v=\"133\", \"Chromium\";v=\"133\"',\n",
    "        '-H', 'sec-ch-ua-arch: \"arm\"',\n",
    "        '-H', 'sec-ch-ua-full-version-list: \"Not(A:Brand\";v=\"99.0.0.0\", \"Google Chrome\";v=\"133.0.6943.127\", \"Chromium\";v=\"133.0.6943.127\"',\n",
    "        '-H', 'sec-ch-ua-mobile: ?0',\n",
    "        '-H', 'sec-ch-ua-model: \"\"',\n",
    "        '-H', 'sec-ch-ua-platform: \"macOS\"',\n",
    "        '-H', 'sec-fetch-dest: document',\n",
    "        '-H', 'sec-fetch-mode: navigate',\n",
    "        '-H', 'sec-fetch-site: none',\n",
    "        '-H', 'sec-fetch-user: ?1',\n",
    "        '-H', 'upgrade-insecure-requests: 1',\n",
    "        '-H', 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(curl_command, capture_output=True, text=True)\n",
    "    headers, _, body = result.stdout.partition(\"\\n\\n\")\n",
    "    \n",
    "    for line in headers.split(\"\\n\"):\n",
    "        if line.lower().startswith(\"location: \"):\n",
    "            redirected_url = line.split(\": \", 1)[1].strip()\n",
    "            print(redirected_url)\n",
    "            return fetch_article(redirected_url)\n",
    "    \n",
    "    return body if result.returncode == 0 else None\n",
    "\n",
    "def parse_archive_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = []\n",
    "    \n",
    "    for link in soup.find_all('a', {'data-testid': 'TitleLink'}):\n",
    "        href = link.get('href')\n",
    "        if href.startswith(\"/article/\"):\n",
    "            articles.append(\"https://www.reuters.com\" + href)\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def parse_article_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    title = soup.find('title').get_text(strip=True) if soup.find('title') else \"\"\n",
    "    \n",
    "    date_info = soup.find_all(class_='date-line__date___kNbY')\n",
    "    date, time_, updated = [d.get_text(strip=True) for d in date_info[:3]] if len(date_info) >= 3 else (\"\", \"\", \"\")\n",
    "    \n",
    "    body = \"\".join([p.get_text(strip=True) for p in soup.find_all(class_='article-body__content__17Yit')])\n",
    "    \n",
    "    tags = [tag.get_text(strip=True) for tag in soup.find_all(attrs={'aria-label': 'Tags'})]\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"date\": date,\n",
    "        \"time\": time_,\n",
    "        \"updated\": updated,\n",
    "        \"body\": body,\n",
    "        \"tags\": tags\n",
    "    }\n",
    "\n",
    "def save_articles_links(current_date, article_links):\n",
    "    # 格式化日期为 YYYYMMDD\n",
    "    date_str = current_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # 设置保存目录和文件路径\n",
    "    output_dir = \"articles_links\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    output_file = os.path.join(output_dir, f\"{date_str}.txt\")\n",
    "    \n",
    "    # 将列表内容写入文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for link in article_links:\n",
    "            f.write(f\"{link}\\n\")\n",
    "    \n",
    "    print(f\"Saved {len(article_links)} links to {output_file}\")\n",
    "\n",
    "def check_google():\n",
    "    try:\n",
    "        # Send get request to google main page\n",
    "        response = requests.get(\"https://www.google.com\", timeout=5)\n",
    "        # check status code = 200\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        # return false if no response\n",
    "        return False\n",
    "\n",
    "def wait_til_run(run_days):\n",
    "    # Have run 2 days before last break\n",
    "    if run_days % 2 == 0:\n",
    "        while True:\n",
    "            # get current minute\n",
    "            current_minute = datetime.now().minute\n",
    "            \n",
    "            # check if current minute is divided by 10\n",
    "            if current_minute % 10 == 0:\n",
    "                print(f\"Current Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, start a new run\")\n",
    "                break\n",
    "\n",
    "            # Check every 5 seconds\n",
    "            print(\"Wait till next run\")\n",
    "            time.sleep(10)\n",
    "\n",
    "    # wait the phone trigger shortcuts\n",
    "    time.sleep(20)\n",
    "    while not check_google():\n",
    "        time.sleep(5)\n",
    "    \n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.reuters.com/archive/{}/{}/{}/\"\n",
    "    start_date = datetime(2021, 4, 6)\n",
    "    end_date = datetime(2021, 5, 31)\n",
    "    current_date = start_date\n",
    "    #time.sleep(3600)\n",
    "    run_days = 0\n",
    "    while current_date <= end_date:\n",
    "        print(f\"Current time:{datetime.now()}\")\n",
    "        print(f\"Run days={run_days}\")\n",
    "\n",
    "        #wait_til_run(run_days)\n",
    "        \n",
    "        yyyy_mm = current_date.strftime(\"%Y-%m\")\n",
    "        dd = current_date.strftime(\"%d\")\n",
    "        print(f'current_date={current_date.strftime(\"%Y-%m-%d\")}')\n",
    "        page = 1\n",
    "\n",
    "        article_links = set([])\n",
    "        retry_sleep_time = 10 # If need retry, time should sleep for a while before retry\n",
    "    \n",
    "        while True:\n",
    "            archive_url = base_url.format(yyyy_mm, dd, page)\n",
    "            html = fetch_page(archive_url)\n",
    "            if not html:\n",
    "                break\n",
    "            \n",
    "            links = parse_archive_page(html)\n",
    "            article_links = article_links | set(links)\n",
    "            \n",
    "            if len(links) < 2:\n",
    "                if page == 1:\n",
    "                    # no enough links for first page\n",
    "                    article_links.clear()\n",
    "                    \n",
    "                    retry_flag = True\n",
    "                    retry_sleep_time += 10 # every time you encounter a block issue, sleep 60 seconds more\n",
    "                    print(f'Sleep for {retry_sleep_time} seconds before retry')\n",
    "                    time.sleep(retry_sleep_time)\n",
    "                    continue\n",
    "                    \n",
    "                print('not links')\n",
    "                break\n",
    "            else:\n",
    "                retry_sleep_time = 60\n",
    "\n",
    "            page += 1\n",
    "            if page % 10 == 0:\n",
    "                print(f'{page} pages for {current_date.strftime(\"%Y-%m-%d\")}, total num is {len(article_links)}')\n",
    "            #time.sleep(0.5)\n",
    "            #time.sleep(random.uniform(1, 2))\n",
    "        print(f'Totally {page} pages for {current_date.strftime(\"%Y-%m-%d\")}, total num is {len(article_links)}')\n",
    "        save_articles_links(current_date, article_links)\n",
    "        \n",
    "        run_days += 1\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9f9a7-4799-44f0-a753-f5df87c7e9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af938d4d-f08a-4d81-8a95-646f2ba7d8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
