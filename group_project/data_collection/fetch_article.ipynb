{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e9f69-c4dd-426a-bb2b-fe858216d99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to process 20210103, Current time:2025-03-03 18:48:20.408000\n",
      "Start to process 100 links with 10 threads...\n",
      "Sent\n",
      "Batch processing completed. Total links processed: 100\n",
      "Start to process 100 links with 10 threads...\n",
      "Sent\n",
      "Batch processing completed. Total links processed: 100\n",
      "Start to process 100 links with 10 threads...\n",
      "Sent\n",
      "Batch processing completed. Total links processed: 100\n",
      "Start to process 47 links with 10 threads...\n",
      "Batch processing completed. Total links processed: 40\n",
      "Day processing completed. Total links processed: 40\n",
      "Start to process 20210104, Current time:2025-03-03 18:51:43.978505\n",
      "Start to process 100 links with 10 threads...\n",
      "Sent\n",
      "Batch processing completed. Total links processed: 100\n",
      "Start to process 100 links with 10 threads...\n",
      "Sent\n",
      "Batch processing completed. Total links processed: 100\n",
      "Start to process 84 links with 10 threads...\n",
      "Batch processing completed. Total links processed: 80\n",
      "Day processing completed. Total links processed: 80\n",
      "Start to process 20210105, Current time:2025-03-03 18:54:25.445200\n",
      "Start to process 67 links with 10 threads...\n",
      "Batch processing completed. Total links processed: 60\n",
      "Day processing completed. Total links processed: 60\n",
      "Start to process 20210106, Current time:2025-03-03 18:54:53.005088\n",
      "Start to process 32 links with 10 threads...\n",
      "Batch processing completed. Total links processed: 30\n",
      "Day processing completed. Total links processed: 30\n",
      "Start to process 20210107, Current time:2025-03-03 18:55:06.357302\n",
      "Start to process 100 links with 10 threads...\n",
      "Sent\n",
      "Batch processing completed. Total links processed: 100\n",
      "Start to process 12 links with 10 threads...\n",
      "Batch processing completed. Total links processed: 10\n",
      "Day processing completed. Total links processed: 10\n",
      "Start to process 20210108, Current time:2025-03-03 18:56:12.018034\n",
      "Start to process 100 links with 10 threads...\n",
      "Sent\n",
      "Batch processing completed. Total links processed: 100\n",
      "Start to process 100 links with 10 threads...\n",
      "Sent\n",
      "Batch processing completed. Total links processed: 100\n",
      "Start to process 100 links with 10 threads...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import threading\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "import imessage\n",
    "import subprocess\n",
    "\n",
    "# random user agent list\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
    "]\n",
    "\n",
    "# input and output directory\n",
    "INPUT_FILE = \"articles_links/20190105.txt\"\n",
    "OUTPUT_DIR = \"reuters_articles/20190105\"\n",
    "LOG_FILE = \"reuters_crawl_log.txt\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "def log_error(message):\n",
    "    '''\n",
    "        Record error message\n",
    "    '''\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\\n\")\n",
    "\n",
    "def read_links(file_path):\n",
    "    '''\n",
    "        Read links from last step.\n",
    "    '''\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            links = [line.strip() for line in f if line.strip()]\n",
    "        return list(set(links))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        log_error(f\"File not found: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def check_google():\n",
    "    try:\n",
    "        # Send get request to google main page\n",
    "        response = requests.get(\"https://www.google.com\", timeout=5)\n",
    "        # check status code = 200\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        # return false if no response\n",
    "        return False\n",
    "\n",
    "def send_switch_ip_msg():\n",
    "    # send iMessage to iPhone\n",
    "    imessage.send(['huiyang.han@gmail.com'], 'Hi Harry!')\n",
    "    print('Sent')\n",
    "    time.sleep(20)\n",
    "    while not check_google():\n",
    "        time.sleep(5)\n",
    "\n",
    "def fetch_page(url, redirect=False):\n",
    "    '''\n",
    "        Get article page content by url.\n",
    "    '''\n",
    "    headers = {\n",
    "        \"User-Agent\": random.choice(USER_AGENTS),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15, allow_redirects=True)\n",
    "        #print(response.text)\n",
    "        #print(response.headers)\n",
    "        #print(f'-----------redirect={redirect}')\n",
    "        if 'Location' in response.headers:\n",
    "            location = response.headers['Location']\n",
    "            return get_page_content(location, True)\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        error_msg = f\"Request failed: {url}, error: {e}\"\n",
    "        log_error(error_msg)\n",
    "        return None\n",
    "\n",
    "def parse_article(html):\n",
    "    '''\n",
    "        Parse article contents.\n",
    "    '''\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Extract title\n",
    "    title = soup.find('title').get_text(strip=True) if soup.find('title') else \"\"\n",
    "    \n",
    "    # Extract date info\n",
    "    date_info = soup.find_all(class_='date-line__date___kNbY')\n",
    "    date, time_, updated = [d.get_text(strip=True) for d in date_info[:3]] if len(date_info) >= 3 else (\"\", \"\", \"\")\n",
    "    \n",
    "    # Extract article body\n",
    "    body = \"\".join([p.get_text(strip=True) for p in soup.find_all(class_='article-body__content__17Yit')])\n",
    "    \n",
    "    # Extract tags and remove 'Suggested Topics:'\n",
    "    tags_raw = [tag.get_text(strip=True) for tag in soup.find_all(attrs={'aria-label': 'Tags'})]\n",
    "    tags = []\n",
    "    for tag in tags_raw:\n",
    "        if tag.startswith(\"Suggested Topics:\"):\n",
    "            cleaned_tag = tag.replace(\"Suggested Topics:\", \"\").strip()\n",
    "            if cleaned_tag:\n",
    "                tags.append(cleaned_tag)\n",
    "        else:\n",
    "            tags.append(tag)\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"date\": date,\n",
    "        \"time\": time_,\n",
    "        \"updated\": updated,\n",
    "        \"body\": body,\n",
    "        \"tags\": tags\n",
    "    }\n",
    "\n",
    "def sanitize_filename(title):\n",
    "    '''\n",
    "        Sanitize the filename for it can contain spaces and other special characters.\n",
    "    '''\n",
    "    invalid_chars = r'[<>:\"/\\\\|?*]'\n",
    "    sanitized = re.sub(invalid_chars, '_', title)\n",
    "    return sanitized[:200]\n",
    "\n",
    "def save_article(article_data, index):\n",
    "    '''\n",
    "        Save article content as a json file.\n",
    "    '''\n",
    "    if not article_data[\"title\"]:\n",
    "        filename = f\"{OUTPUT_DIR}/article_{index:04d}.json\"\n",
    "    else:\n",
    "        sanitized_title = sanitize_filename(article_data[\"title\"])\n",
    "        filename = f\"{OUTPUT_DIR}/{sanitized_title}.json\"\n",
    "    \n",
    "    base_filename = filename\n",
    "    counter = 1\n",
    "    while os.path.exists(filename):\n",
    "        filename = f\"{base_filename[:-5]}_{counter}.json\"\n",
    "        counter += 1\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(article_data, f, ensure_ascii=False, indent=4)\n",
    "    #print(f\"Saved article: {article_data['title']} to {filename}\")\n",
    "\n",
    "class ThreadController:\n",
    "    def __init__(self):\n",
    "        self.pause_event = threading.Event()\n",
    "        self.pause_event.set()\n",
    "        self.processed_count = 0\n",
    "        self.count_lock = Lock()\n",
    "        \n",
    "    def pause_all(self):\n",
    "        \"\"\"\n",
    "            Pause all the threads.\n",
    "        \"\"\"\n",
    "        self.pause_event.clear()\n",
    "        \n",
    "    def resume_all(self):\n",
    "        \"\"\"\n",
    "            Resume all the threads.\n",
    "        \"\"\"\n",
    "        self.pause_event.set()\n",
    "        \n",
    "    def check_pause(self):\n",
    "        \"\"\"\n",
    "            Check whther to pause.\n",
    "        \"\"\"\n",
    "        self.pause_event.wait()\n",
    "        \n",
    "    def increment_count(self):\n",
    "        \"\"\"\n",
    "            Increase the number thread-safely.\n",
    "        \"\"\"\n",
    "        with self.count_lock:\n",
    "            self.processed_count += 1\n",
    "            if self.processed_count % 100 == 0:\n",
    "                self.pause_all()\n",
    "                send_switch_ip_msg()\n",
    "                self.resume_all()\n",
    "            return self.processed_count\n",
    "    \n",
    "\n",
    "def process_chunk(links_chunk, thread_id, controller):\n",
    "    \"\"\"\n",
    "        Process link chunks.\n",
    "    \"\"\"\n",
    "    for i, link in enumerate(links_chunk, 1):\n",
    "        # Check whether to pause\n",
    "        controller.check_pause()\n",
    "        #print(f'link={link}')\n",
    "\n",
    "        # Update the link numbers handled.\n",
    "        controller.increment_count()     \n",
    "        \n",
    "        try:\n",
    "            save_flag = False\n",
    "            html = fetch_page(link)\n",
    "            \n",
    "            if html and html.find('Please enable JS') < 0:\n",
    "                article_data = parse_article(html)\n",
    "                save_article(article_data, i)\n",
    "                save_flag = True\n",
    "            else:\n",
    "                # retry for 5 times\n",
    "                retry_times = 0\n",
    "                while retry_times < 5:\n",
    "                    retry_times += 1\n",
    "                    time.sleep(0.5)\n",
    "                    html = fetch_page(link)\n",
    "                    if html and html.find('Please enable JS') < 0:\n",
    "                        article_data = parse_article(html)\n",
    "                        save_article(article_data, i)\n",
    "                        save_flag = True\n",
    "                        break\n",
    "                #if not save_flag:\n",
    "                    #print(f'Not saving {link}')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Thread {thread_id}: Error processing link {link}: {e}\")\n",
    "            continue\n",
    "\n",
    "def process_articles():\n",
    "    '''\n",
    "        Process all the article links with 10 parallel threads.\n",
    "    '''\n",
    "    links = read_links(INPUT_FILE)\n",
    "    #links = links[0:10]\n",
    "    if not links:\n",
    "        print(\"No link found, exit the process\")\n",
    "        return\n",
    "    \n",
    "    random.shuffle(links)\n",
    "\n",
    "    batch_size = 100\n",
    "    total_num = 0\n",
    "    for i in range(0, len(links), batch_size):\n",
    "        # Get current batch by slicing.\n",
    "        batch = links[i:i + batch_size]\n",
    "        \n",
    "        # Split this batch to 10 sub batch.\n",
    "        chunk_size = max(1, len(batch) // 10)\n",
    "        link_chunks = [batch[i:i + chunk_size] for i in range(0, len(batch), chunk_size)]\n",
    "        \n",
    "        print(f\"Start to process {len(batch)} links with 10 threads...\")\n",
    "        \n",
    "        # New a thread controller\n",
    "        controller = ThreadController()\n",
    "        \n",
    "        # Use ThreadPoolExecutor to handle 10 threads\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = [\n",
    "                executor.submit(process_chunk, chunk, idx, controller)\n",
    "                for idx, chunk in enumerate(link_chunks[:10])  # 确保不超过10个线程\n",
    "            ]\n",
    "            \n",
    "            # Wait till all the threads end\n",
    "            for future in futures:\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Thread execution failed: {e}\")\n",
    "        # Output the final result\n",
    "        print(f\"Batch processing completed. Total links processed: {controller.processed_count}\")\n",
    "        total_num += controller.processed_count\n",
    "    print(f\"Day processing completed. Total links processed: {controller.processed_count}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # start date\n",
    "    start_date = datetime(2021, 1, 3)\n",
    "    # end date\n",
    "    end_date = datetime(2021, 1, 31)\n",
    "    # iteration date\n",
    "    current_date = start_date\n",
    "    \n",
    "    #send_switch_ip_msg()\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y%m%d\")\n",
    "        print(f'Start to process {date_str}, Current time:{datetime.now()}')\n",
    "\n",
    "        # construct article link file\n",
    "        INPUT_FILE = f\"articles_links/{date_str}.txt\"\n",
    "        # create article contents directory\n",
    "        OUTPUT_DIR = f\"reuters_articles/{date_str}\"\n",
    "    \n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "            os.makedirs(OUTPUT_DIR)\n",
    "        process_articles()\n",
    "    \n",
    "        # Iterate the date\n",
    "        current_date += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616bf8c-106d-4bac-9cd3-30a4e56d9596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988cada-c54e-490d-ab83-1df21217b3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
